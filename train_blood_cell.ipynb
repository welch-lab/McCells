{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/jingqiao/real_McCell/data/processed/10-24 for date 2025-10-24\n",
      "Loading cached ontology from /home/jingqiao/real_McCell/data/processed/ontology.pkl...\n",
      "Ontology loaded successfully.\n",
      "\n",
      "All data artifacts loaded successfully.\n",
      "Loaded 80 cell types.\n",
      "  - 23 leaf nodes\n",
      "  - 57 internal nodes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Imports from our project\n",
    "from src.utils.paths import PROJECT_ROOT, get_data_folder\n",
    "from src.utils.ontology_utils import load_ontology  # Still need this to access term names\n",
    "\n",
    "# --- 1. Load Preprocessed Data Artifacts ---\n",
    "# Instead of running preprocessing, we now load the files created by `run_preprocessing.py`.\n",
    "\n",
    "# Hardcoded date for loading the preprocessed files\n",
    "DATE = '2025-10-24'\n",
    "PROCESSED_DATA_DIR = get_data_folder(DATE)\n",
    "\n",
    "print(f\"Loading data from: {PROCESSED_DATA_DIR} for date {DATE}\")\n",
    "\n",
    "# Load the ontology object to get term names for printing\n",
    "cl = load_ontology()\n",
    "\n",
    "# Load DataFrames\n",
    "marginalization_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_marginalization_df.csv\", index_col=0)\n",
    "parent_child_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_parent_child_df.csv\", index_col=0)\n",
    "exclusion_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_exclusion_df.csv\", index_col=0)\n",
    "\n",
    "# Load mapping_dict\n",
    "mapping_dict_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_mapping_dict_df.csv\", index_col=0)\n",
    "# The DataFrame was saved with CL numbers as the index and integer mappings in the first column\n",
    "mapping_dict = pd.Series(mapping_dict_df.iloc[:, 0].values, index=mapping_dict_df.index).to_dict()\n",
    "\n",
    "# Load leaf and internal values\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_leaf_values.pkl\", \"rb\") as fp:\n",
    "    leaf_values = pickle.load(fp)\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_internal_values.pkl\", \"rb\") as fp:\n",
    "    internal_values = pickle.load(fp)\n",
    "\n",
    "print(\"\\nAll data artifacts loaded successfully.\")\n",
    "print(f\"Loaded {len(mapping_dict)} cell types.\")\n",
    "print(f\"  - {len(leaf_values)} leaf nodes\")\n",
    "print(f\"  - {len(internal_values)} internal nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein-coding genes from BioMart...\n",
      "Loaded 23262 protein-coding genes from BioMart\n",
      "Ready to query 80 cell types and 23262 protein-coding genes.\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "import tiledbsoma as soma\n",
    "from tiledbsoma_ml import ExperimentDataset, experiment_dataloader\n",
    "import pandas as pd\n",
    "\n",
    "# Get all cell types from our mapping dict to build the query\n",
    "all_cell_values = list(mapping_dict.keys())\n",
    "\n",
    "# --- Load gene list from BioMart (matching old_reference approach) ---\n",
    "print(\"Loading protein-coding genes from BioMart...\")\n",
    "biomart_path = PROJECT_ROOT / \"hpc_workaround/data/mart_export.txt\"\n",
    "biomart = pd.read_csv(biomart_path)\n",
    "\n",
    "# Filter for protein-coding genes only\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "gene_list = coding_only['Gene stable ID'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(gene_list)} protein-coding genes from BioMart\")\n",
    "\n",
    "# Create the 'value_filter' strings for the query\n",
    "var_value_filter = f\"feature_id in {gene_list}\"\n",
    "obs_value_filter = f'assay == \"10x 3\\' v3\" and is_primary_data == True and cell_type_ontology_term_id in {all_cell_values}'\n",
    "\n",
    "print(f\"Ready to query {len(all_cell_values)} cell types and {len(gene_list)} protein-coding genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening local SOMA database at: /scratch/sigbio_project_root/sigbio_project25/jingqiao/mccell-single/soma_db_homo_sapiens\n",
      "\n",
      "Total matching cells: 24582\n",
      "Training set size: 19665\n",
      "Validation set size: 4917\n",
      "\n",
      "Train dataset shape: (19665, 20060)\n",
      "Validation dataset shape: (4917, 20060)\n"
     ]
    }
   ],
   "source": [
    "# Point to the local SOMA database (which is already the homo_sapiens experiment)\n",
    "soma_uri = \"/scratch/sigbio_project_root/sigbio_project25/jingqiao/mccell-single/soma_db_homo_sapiens\"\n",
    "print(f\"Opening local SOMA database at: {soma_uri}\")\n",
    "\n",
    "# Open the experiment directly (it's a SOMAExperiment, not a SOMACollection)\n",
    "experiment = soma.open(soma_uri, mode=\"r\")\n",
    "\n",
    "# Create the ExperimentDataset and DataLoaders using the query filters\n",
    "with experiment.axis_query(\n",
    "    measurement_name=\"RNA\",\n",
    "    obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "    var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    ") as query:\n",
    "    experiment_dataset = ExperimentDataset(\n",
    "        query,\n",
    "        obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "        layer_name=\"normalized\",\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        seed=111\n",
    "    )\n",
    "\n",
    "    train_dataset, val_dataset = experiment_dataset.random_split(0.8, 0.2, seed=42)\n",
    "\n",
    "    print(f'\\nTotal matching cells: {len(experiment_dataset)}')\n",
    "    print(f'Training set size: {len(train_dataset)}')\n",
    "    print(f'Validation set size: {len(val_dataset)}')\n",
    "\n",
    "    train_dataloader = experiment_dataloader(train_dataset)\n",
    "    val_dataloader = experiment_dataloader(val_dataset)\n",
    "\n",
    "# Show a summary of the loaded train and validation datasets\n",
    "print(\"\\nTrain dataset shape:\", train_dataset.shape)\n",
    "print(\"Validation dataset shape:\", val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEBUG: Filter strings\n",
      "================================================================================\n",
      "Number of cell types: 80\n",
      "Number of genes: 23262\n",
      "\n",
      "obs_value_filter (first 200 chars):\n",
      "assay == \"10x 3' v3\" and is_primary_data == True and cell_type_ontology_term_id in ['CL:0000233', 'CL:0000895', 'CL:0000900', 'CL:0000904', 'CL:0000905', 'CL:0000910', 'CL:0000912', 'CL:0000913', 'CL:...\n",
      "\n",
      "var_value_filter (first 200 chars):\n",
      "feature_id in ['ENSG00000198888', 'ENSG00000198763', 'ENSG00000198804', 'ENSG00000198712', 'ENSG00000228253', 'ENSG00000198899', 'ENSG00000198938', 'ENSG00000198840', 'ENSG00000212907', 'ENSG000001988...\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Print the actual filter strings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUG: Filter strings\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of cell types: {len(all_cell_values)}\")\n",
    "print(f\"Number of genes: {len(gene_list)}\")\n",
    "print(f\"\\nobs_value_filter (first 200 chars):\")\n",
    "print(obs_value_filter[:200] + \"...\")\n",
    "print(f\"\\nvar_value_filter (first 200 chars):\")\n",
    "print(var_value_filter[:200] + \"...\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEBUG: Testing query directly\n",
      "================================================================================\n",
      "Cells in query (before ExperimentDataset): 6,292,764\n",
      "Unique cell types: 80\n",
      "================================================================================\n",
      "\n",
      "Available layers in RNA measurement:\n",
      "['normalized', 'raw']\n",
      "\n",
      "================================================================================\n",
      "DEBUG: Checking layer coverage\n",
      "================================================================================\n",
      "Query expects:\n",
      "  Cells (obs): 6,292,764\n",
      "  Genes (var): 20,060\n",
      "\n",
      "Checking 'raw' layer...\n",
      "  First chunk columns: ['soma_dim_0', 'soma_dim_1', 'soma_data']\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Test the query directly before ExperimentDataset wraps it\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUG: Testing query directly\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with experiment.axis_query(\n",
    "  measurement_name=\"RNA\",\n",
    "  obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "  var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    ") as query:\n",
    "  # Just count the cells like in the diagnosis notebook\n",
    "  obs_test = query.obs(column_names=[\"cell_type_ontology_term_id\"]).concat().to_pandas()\n",
    "  print(f\"Cells in query (before ExperimentDataset): {len(obs_test):,}\")\n",
    "  print(f\"Unique cell types: {obs_test['cell_type_ontology_term_id'].nunique()}\")\n",
    "  del obs_test\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check available layers\n",
    "print(\"Available layers in RNA measurement:\")\n",
    "print(list(experiment.ms[\"RNA\"].X.keys()))\n",
    "\n",
    "# Check layer coverage properly\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUG: Checking layer coverage\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with experiment.axis_query(\n",
    "  measurement_name=\"RNA\",\n",
    "  obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "  var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    ") as query:\n",
    "  # Get obs and var joinids to see the expected dimensions\n",
    "  obs_joinids = query.obs_joinids()\n",
    "  var_joinids = query.var_joinids()\n",
    "\n",
    "  print(f\"Query expects:\")\n",
    "  print(f\"  Cells (obs): {len(obs_joinids):,}\")\n",
    "  print(f\"  Genes (var): {len(var_joinids):,}\")\n",
    "\n",
    "  # Try to iterate over raw layer\n",
    "  print(\"\\nChecking 'raw' layer...\")\n",
    "  raw_chunks = 0\n",
    "  raw_nnz = 0\n",
    "  for table in query.X(\"raw\").tables():\n",
    "      raw_chunks += 1\n",
    "      raw_nnz += len(table)\n",
    "      if raw_chunks == 1:\n",
    "          print(f\"  First chunk columns: {table.column_names}\")\n",
    "  print(f\"  Chunks read: {raw_chunks}\")\n",
    "  print(f\"  Non-zero entries: {raw_nnz:,}\")\n",
    "\n",
    "  # Try normalized layer\n",
    "  print(\"\\nChecking 'normalized' layer...\")\n",
    "  norm_chunks = 0\n",
    "  norm_nnz = 0\n",
    "  for table in query.X(\"normalized\").tables():\n",
    "      norm_chunks += 1\n",
    "      norm_nnz += len(table)\n",
    "      if norm_chunks == 1:\n",
    "          print(f\"  First chunk columns: {table.column_names}\")\n",
    "  print(f\"  Chunks read: {norm_chunks}\")\n",
    "  print(f\"  Non-zero entries: {norm_nnz:,}\")\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Model, optimizer, and loss function are ready.\n"
     ]
    }
   ],
   "source": [
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Setup ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# The input dimension is the number of genes from our dataset object\n",
    "input_dim = train_dataset.shape[1]\n",
    "output_dim = len(leaf_values)  # Model only predicts leaf nodes\n",
    "\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Instantiate the new, correct loss function with all required artifacts\n",
    "loss_fn = MarginalizationLoss(\n",
    "    marginalization_df=marginalization_df,\n",
    "    parent_child_df=parent_child_df,\n",
    "    exclusion_df=exclusion_df,\n",
    "    leaf_values=leaf_values,\n",
    "    internal_values=internal_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Model, optimizer, and loss function are ready.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "batches_per_epoch = 200\n",
    "batch_loss_history = []\n",
    "\n",
    "print(f\"\\nStarting training for {num_epochs} epochs ({batches_per_epoch} batches each)...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f'\\n--- Epoch {epoch + 1} ---')\n",
    "\n",
    "    for i, (X_batch, obs_batch) in enumerate(train_dataloader):\n",
    "        if i >= batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        # Data preparation\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        X_batch = torch.log1p(X_batch)  # Log-transform gene expression\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "        y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch)\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        batch_loss_history.append(total_loss.item())\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'  [Batch {i + 1:3d}] Total Loss: {total_loss.item():.4f} (Leaf: {loss_leafs.item():.4f}, Parent: {loss_parents.item():.4f})')\n",
    "\n",
    "print('\\nFinished Training.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Plot Training Loss ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_loss_history, alpha=0.6, linewidth=0.8)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Training Loss per Batch')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Moving average for smoother trend\n",
    "window_size = 20\n",
    "if len(batch_loss_history) >= window_size:\n",
    "    moving_avg = np.convolve(batch_loss_history, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(batch_loss_history)), moving_avg, \n",
    "             color='red', linewidth=2, label=f'{window_size}-batch moving avg')\n",
    "    plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot loss by epoch (if multiple epochs)\n",
    "if num_epochs > 1:\n",
    "    epoch_losses = [batch_loss_history[i*batches_per_epoch:(i+1)*batches_per_epoch] \n",
    "                   for i in range(num_epochs)]\n",
    "    for i, losses in enumerate(epoch_losses):\n",
    "        plt.plot(losses, label=f'Epoch {i+1}', alpha=0.7)\n",
    "    plt.xlabel('Batch (within epoch)')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('Training Loss by Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Run multiple epochs\\nto see comparison', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial loss: {batch_loss_history[0]:.4f}\")\n",
    "print(f\"  Final loss: {batch_loss_history[-1]:.4f}\")\n",
    "print(f\"  Loss reduction: {(1 - batch_loss_history[-1]/batch_loss_history[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- Validate the Model (LEAF NODES ONLY) ---\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION (LEAF NODES ONLY)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "val_batches = 50\n",
    "\n",
    "val_total_losses = []\n",
    "val_leaf_losses = []\n",
    "val_parent_losses = []\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "leaf_indices_set = {mapping_dict[cid] for cid in leaf_values}\n",
    "total_samples = 0\n",
    "leaf_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (X_batch, obs_batch) in enumerate(val_dataloader):\n",
    "        if i >= val_batches:\n",
    "            break\n",
    "\n",
    "        # Data preparation\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        X_batch = torch.log1p(X_batch)\n",
    "        X_batch = X_batch.to(device)\n",
    "\n",
    "        label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "        y_batch = torch.tensor([mapping_dict[term] for term in label_strings],\n",
    "                              device=device, dtype=torch.long)\n",
    "\n",
    "        total_samples += len(y_batch)\n",
    "\n",
    "        # FILTER: Only keep samples with LEAF node labels\n",
    "        is_leaf = torch.tensor([y.item() in leaf_indices_set for y in y_batch], device=device)\n",
    "\n",
    "        if is_leaf.sum() == 0:\n",
    "            continue  # Skip batches with no leaf samples\n",
    "\n",
    "        X_batch_leaf = X_batch[is_leaf]\n",
    "        y_batch_leaf = y_batch[is_leaf]\n",
    "        leaf_samples += len(y_batch_leaf)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch_leaf)\n",
    "        total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch_leaf)\n",
    "\n",
    "        val_total_losses.append(total_loss.item())\n",
    "        val_leaf_losses.append(loss_leafs.item())\n",
    "        val_parent_losses.append(loss_parents.item())\n",
    "\n",
    "        # Get predictions (argmax of logits)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(y_batch_leaf.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "avg_val_loss = np.mean(val_total_losses)\n",
    "avg_leaf_loss = np.mean(val_leaf_losses)\n",
    "avg_parent_loss = np.mean(val_parent_losses)\n",
    "\n",
    "# Accuracy (only meaningful for leaf nodes)\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "accuracy = (all_predictions == all_labels).mean()\n",
    "\n",
    "print(f\"\\nValidation Results (LEAF-LABELED SAMPLES ONLY):\")\n",
    "print(f\"  Total samples processed: {total_samples}\")\n",
    "print(f\"  Leaf-labeled samples: {leaf_samples} ({leaf_samples/total_samples*100:.1f}%)\")\n",
    "print(f\"  Internal-labeled samples (skipped): {total_samples - leaf_samples}\")\n",
    "print(f\"\\n  Average Total Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"  Average Leaf Loss:  {avg_leaf_loss:.4f}\")\n",
    "print(f\"  Average Parent Loss: {avg_parent_loss:.4f}\")\n",
    "print(f\"  Leaf Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"\\nNote: This validation only tests the model on leaf node predictions.\")\n",
    "print(f\"      Internal node labels are excluded from evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check leaf sample distribution in train vs validation\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING TRAIN VS VALIDATION LEAF DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "leaf_indices_set = {mapping_dict[cid] for cid in leaf_values}\n",
    "\n",
    "# Count in training data\n",
    "train_total = 0\n",
    "train_leaf = 0\n",
    "train_leaf_labels = []\n",
    "\n",
    "for i, (X_batch, obs_batch) in enumerate(train_dataloader):\n",
    "    if i >= 100:  # Sample first 100 batches\n",
    "        break\n",
    "    label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "    y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "    \n",
    "    train_total += len(y_batch)\n",
    "    is_leaf = torch.tensor([y.item() in leaf_indices_set for y in y_batch])\n",
    "    train_leaf += is_leaf.sum().item()\n",
    "    train_leaf_labels.extend(y_batch[is_leaf].tolist())\n",
    "\n",
    "# Count in validation data\n",
    "val_total = 0\n",
    "val_leaf = 0\n",
    "val_leaf_labels = []\n",
    "\n",
    "for i, (X_batch, obs_batch) in enumerate(val_dataloader):\n",
    "    if i >= 50:  # All validation batches\n",
    "        break\n",
    "    label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "    y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "    \n",
    "    val_total += len(y_batch)\n",
    "    is_leaf = torch.tensor([y.item() in leaf_indices_set for y in y_batch])\n",
    "    val_leaf += is_leaf.sum().item()\n",
    "    val_leaf_labels.extend(y_batch[is_leaf].tolist())\n",
    "\n",
    "print(f\"\\nTRAINING DATA (first 100 batches):\")\n",
    "print(f\"  Total samples: {train_total}\")\n",
    "print(f\"  Leaf samples: {train_leaf} ({train_leaf/train_total*100:.1f}%)\")\n",
    "print(f\"  Internal samples: {train_total - train_leaf} ({(train_total-train_leaf)/train_total*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nVALIDATION DATA (first 50 batches):\")\n",
    "print(f\"  Total samples: {val_total}\")\n",
    "print(f\"  Leaf samples: {val_leaf} ({val_leaf/val_total*100:.1f}%)\")\n",
    "print(f\"  Internal samples: {val_total - val_leaf} ({(val_total-val_leaf)/val_total*100:.1f}%)\")\n",
    "\n",
    "# Check which specific leaf types appear\n",
    "from collections import Counter\n",
    "train_leaf_counts = Counter(train_leaf_labels)\n",
    "val_leaf_counts = Counter(val_leaf_labels)\n",
    "\n",
    "print(f\"\\n\\nUnique leaf types in training: {len(train_leaf_counts)}\")\n",
    "print(f\"Unique leaf types in validation: {len(val_leaf_counts)}\")\n",
    "\n",
    "# Check if validation has leaf types NOT in training\n",
    "val_only_leaves = set(val_leaf_counts.keys()) - set(train_leaf_counts.keys())\n",
    "if val_only_leaves:\n",
    "    print(f\"\\n⚠️  WARNING: {len(val_only_leaves)} leaf types appear in validation but NOT in training!\")\n",
    "    print(f\"  These leaf types: {list(val_only_leaves)[:5]}\")\n",
    "else:\n",
    "    print(f\"\\n✓ All validation leaf types also appear in training\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which leaf types are missing from training?\n",
    "reverse_mapping = {v: k for k, v in mapping_dict.items()}\n",
    "missing_leaves = [24, 4, 14]\n",
    "\n",
    "print(\"Missing leaf types (in validation but not training):\")\n",
    "for idx in missing_leaves:\n",
    "    cl_id = reverse_mapping[idx]\n",
    "    print(f\"  Index {idx}: {cl_id} - {cl[cl_id].name}\")\n",
    "\n",
    "# Check how many samples of each in validation\n",
    "from collections import Counter\n",
    "val_leaf_counts = Counter(val_leaf_labels)\n",
    "print(f\"\\nSample counts in validation:\")\n",
    "for idx in missing_leaves:\n",
    "    count = val_leaf_counts.get(idx, 0)\n",
    "    print(f\"  {cl[reverse_mapping[idx]].name}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "McCell",
   "language": "python",
   "name": "mccell"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
