{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached ontology from /Users/jzhao/dev/Welch-lab/McCell/data/processed/ontology.pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-01-30. Specify 'census_version=\"2025-01-30\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology loaded successfully.\n",
      "Fetching descendants of CL:0000988...\n",
      "Connecting to CellXGene Census...\n",
      "Reading cell metadata to filter cell types...\n",
      "Found 160 cell types with > 5000 cells.\n",
      "Querying for final cell metadata...\n",
      "Finished loading and filtering cell metadata.\n",
      "141 cell types in the dataset 41 leaf types, 100 internal types\n",
      "Loaded 141 cell types.\n",
      "Cell children mask shape: torch.Size([141, 141])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from src.utils.ontology_utils import load_ontology\n",
    "from src.data_pipeline.data_loader import load_filtered_cell_metadata\n",
    "from src.data_pipeline.preprocess_ontology import preprocess_data_ontology\n",
    "from src.utils.paths import PROJECT_ROOT\n",
    "from src.utils.ontology_utils import get_sub_DAG\n",
    "\n",
    "# 1. Load the cached ontology object\n",
    "cl = load_ontology()\n",
    "\n",
    "# Define the root of the ontology subgraph to be processed\n",
    "root_cl_id = 'CL:0000988'  # hematopoietic cell\n",
    "\n",
    "# 2. Load filtered cell metadata from CellXGene Census\n",
    "# This step can take a few minutes , and might not work on hpc due to likely RAM issues\n",
    "cell_obs_metadata = load_filtered_cell_metadata(cl, root_cl_id=root_cl_id)\n",
    "\n",
    "# 3. Preprocess the ontology and cell data\n",
    "target_column = 'cell_type_ontology_term_id'\n",
    "\n",
    "mapping_dict, leaf_values, internal_values, ontology_df, cell_children_mask = preprocess_data_ontology(\n",
    "        cl, cell_obs_metadata, target_column,\n",
    "        upper_limit=root_cl_id,\n",
    "        cl_only=True, include_leafs=False\n",
    "    )\n",
    "\n",
    "reverse_mapping_dict = {v: k for k, v in mapping_dict.items()}\n",
    "\n",
    "print(f'Loaded {len(mapping_dict)} cell types.')\n",
    "print(\"Cell children mask shape:\", cell_children_mask.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellxgene_census\n",
    "import cellxgene_census.experimental.ml as census_ml\n",
    "import tiledbsoma as soma\n",
    "from tiledbsoma_ml import ExperimentDataset, experiment_dataloader\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "all_cell_values = list(mapping_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filtering gene list...\n",
      "Querying 141 cell types and 500 protein-coding genes.\n"
     ]
    }
   ],
   "source": [
    "# --- Gene Filter (same as before) ---\n",
    "print(\"Loading and filtering gene list...\")\n",
    "biomart = pd.read_csv(\"/Users/jzhao/dev/Welch-lab/McCell/hpc_workaround/data/mart_export.txt\")\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "full_gene_list = coding_only['Gene stable ID'].tolist()\n",
    "num_genes_to_use = 500\n",
    "gene_list = full_gene_list[:num_genes_to_use]\n",
    "var_value_filter = f\"feature_id in {gene_list}\"\n",
    "\n",
    "# Create filters for the data query, including the assay and primary data filters.\n",
    "var_value_filter = f\"feature_id in {gene_list}\"\n",
    "obs_value_filter = f'''assay == \"10x 3' v3\" and is_primary_data == True and cell_type_ontology_term_id in {all_cell_values}'''\n",
    "\n",
    "print(f\"Querying {len(all_cell_values)} cell types and {len(gene_list)} protein-coding genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The \"stable\" release is currently 2025-01-30. Specify 'census_version=\"2025-01-30\"' in future calls to open_soma() to ensure data consistency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total matching cells: 49872\n",
      "Training set size: 39898\n",
      "Validation set size: 9975\n",
      "\n",
      "Training and validation DataLoaders are ready.\n"
     ]
    }
   ],
   "source": [
    "with cellxgene_census.open_soma() as census:\n",
    "    experiment = census[\"census_data\"][\"homo_sapiens\"]\n",
    "\n",
    "    # 1. Create an ExperimentDataset with the full query\n",
    "    with experiment.axis_query(\n",
    "        measurement_name=\"RNA\",\n",
    "        obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "        var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    "    ) as query:\n",
    "        experiment_dataset = ExperimentDataset(\n",
    "            query,\n",
    "            obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "            layer_name=\"raw\",\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            seed=111\n",
    "        )\n",
    "\n",
    "        # 2. Split the Dataset object\n",
    "        train_dataset, val_dataset = experiment_dataset.random_split([0.8, 0.2], seed=42)\n",
    "\n",
    "        print(f\"\\nTotal matching cells: {len(experiment_dataset)}\")\n",
    "        print(f\"Training set size: {len(train_dataset)}\")\n",
    "        print(f\"Validation set size: {len(val_dataset)}\")\n",
    "\n",
    "        # 3. Wrap the subsets in the dataloader\n",
    "        train_dataloader = experiment_dataloader(\n",
    "            train_dataset\n",
    "        )\n",
    "        val_dataloader = experiment_dataloader(\n",
    "            val_dataset\n",
    "        )\n",
    "\n",
    "print(\"\\nTraining and validation DataLoaders are ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Setup ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# The input dimension is the number of genes from our dataset object\n",
    "input_dim = train_dataset.shape[1]\n",
    "output_dim = len(leaf_values)\n",
    "\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "loss_fn = MarginalizationLoss(\n",
    "    ontology_df=ontology_df,\n",
    "    leaf_values=leaf_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting a short training run for 1 epochs (1 batches each)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m model.train()\n\u001b[32m      8\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_batch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches_per_epoch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:42\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     data = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.collate_fn(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/tiledbsoma_ml/dataset.py:366\u001b[39m, in \u001b[36mExperimentDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.x_locator.open() \u001b[38;5;28;01mas\u001b[39;00m (X, obs):\n\u001b[32m    354\u001b[39m     io_batch_iter = IOBatchIterable(\n\u001b[32m    355\u001b[39m         chunks=chunks,\n\u001b[32m    356\u001b[39m         io_batch_size=\u001b[38;5;28mself\u001b[39m.io_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m         use_eager_fetch=\u001b[38;5;28mself\u001b[39m.use_eager_fetch,\n\u001b[32m    364\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m MiniBatchIterable(\n\u001b[32m    367\u001b[39m         io_batch_iter=io_batch_iter,\n\u001b[32m    368\u001b[39m         batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m    369\u001b[39m         use_eager_fetch=\u001b[38;5;28mself\u001b[39m.use_eager_fetch,\n\u001b[32m    370\u001b[39m         return_sparse_X=\u001b[38;5;28mself\u001b[39m.return_sparse_X,\n\u001b[32m    371\u001b[39m     )\n\u001b[32m    373\u001b[39m \u001b[38;5;28mself\u001b[39m.epoch += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/tiledbsoma_ml/_eager_iter.py:39\u001b[39m, in \u001b[36mEagerIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# `.cancel` returned false, so the preload is already running.\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Just wait for it.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preload_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m._cleanup()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batches_per_epoch = 1  # Limit to 500 batches per epoch for speed\n",
    "batch_loss_history = []\n",
    "\n",
    "print(f\"\\nStarting a short training run for {num_epochs} epochs ({batches_per_epoch} batches each)...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (X_batch, obs_batch) in enumerate(train_dataloader):\n",
    "        if i >= batches_per_epoch:\n",
    "            break\n",
    "        \n",
    "        # Convert numpy array from dataloader to a torch tensor\n",
    "        X_batch = torch.from_numpy(X_batch).float().to(device)\n",
    "        \n",
    "        # Get label strings from the pandas DataFrame and map to integer indices\n",
    "        label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "        y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "\n",
    "        # Standard PyTorch training steps\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        total_loss, _, _ = loss_fn(outputs, y_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record loss for plotting\n",
    "        batch_loss_history.append(total_loss.item())\n",
    "        \n",
    "        running_loss += total_loss.item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "--- Starting verification run with a single batch ---\n",
      "Fetching the first batch... (This may take a moment)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m model.train()\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# Get just one batch from the dataloader\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     X_batch, obs_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBatch successfully loaded!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# --- Data Preparation ---\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:42\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     data = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.collate_fn(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/tiledbsoma_ml/dataset.py:366\u001b[39m, in \u001b[36mExperimentDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.x_locator.open() \u001b[38;5;28;01mas\u001b[39;00m (X, obs):\n\u001b[32m    354\u001b[39m     io_batch_iter = IOBatchIterable(\n\u001b[32m    355\u001b[39m         chunks=chunks,\n\u001b[32m    356\u001b[39m         io_batch_size=\u001b[38;5;28mself\u001b[39m.io_batch_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m         use_eager_fetch=\u001b[38;5;28mself\u001b[39m.use_eager_fetch,\n\u001b[32m    364\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m MiniBatchIterable(\n\u001b[32m    367\u001b[39m         io_batch_iter=io_batch_iter,\n\u001b[32m    368\u001b[39m         batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m    369\u001b[39m         use_eager_fetch=\u001b[38;5;28mself\u001b[39m.use_eager_fetch,\n\u001b[32m    370\u001b[39m         return_sparse_X=\u001b[38;5;28mself\u001b[39m.return_sparse_X,\n\u001b[32m    371\u001b[39m     )\n\u001b[32m    373\u001b[39m \u001b[38;5;28mself\u001b[39m.epoch += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/Welch-lab/McCell/.venv/lib/python3.11/site-packages/tiledbsoma_ml/_eager_iter.py:39\u001b[39m, in \u001b[36mEagerIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.iterator)\n\u001b[32m     37\u001b[39m     \u001b[38;5;66;03m# `.cancel` returned false, so the preload is already running.\u001b[39;00m\n\u001b[32m     38\u001b[39m     \u001b[38;5;66;03m# Just wait for it.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_preload_future\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28mself\u001b[39m._cleanup()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-aarch64-none/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Setup ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "input_dim = train_dataset.shape[1]\n",
    "output_dim = len(leaf_values)\n",
    "\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "loss_fn = MarginalizationLoss(\n",
    "    ontology_df=ontology_df,\n",
    "    leaf_values=leaf_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# --- 2. Verification Run (1 Batch) ---\n",
    "print(\"\\n--- Starting verification run with a single batch ---\")\n",
    "print(\"Fetching the first batch... (This may take a moment)\")\n",
    "\n",
    "model.train()\n",
    "try:\n",
    "    # Get just one batch from the dataloader\n",
    "    X_batch, obs_batch = next(iter(train_dataloader))\n",
    "    print(\"Batch successfully loaded!\")\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    X_batch = torch.from_numpy(X_batch).float().to(device)\n",
    "    label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "    y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "    print(f\"Data converted to tensors. X_batch shape: {X_batch.shape}\")\n",
    "\n",
    "    # --- Training Step ---\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_batch)\n",
    "    total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch)\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Forward and backward pass completed successfully.\")\n",
    "    \n",
    "    print(\"\\n--- VERIFICATION SUCCESSFUL ---\")\n",
    "    print(f\"Calculated Total Loss: {total_loss.item():.4f}\")\n",
    "    print(f\"  - Leaf Loss component: {loss_leafs.item():.4f}\")\n",
    "    print(f\"  - Parent Loss component: {loss_parents.item():.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- VERIFICATION FAILED ---\")\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.92.136.210\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostbyname(\"cellxgene-census-public-us-west-2.s3.us-west-2.amazonaws.com\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "McCell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
