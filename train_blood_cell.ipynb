{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/jingqiao/real_McCell/data/processed/10-24 for date 2025-10-24\n",
      "Loading cached ontology from /home/jingqiao/real_McCell/data/processed/ontology.pkl...\n",
      "Ontology loaded successfully.\n",
      "\n",
      "All data artifacts loaded successfully.\n",
      "Loaded 80 cell types.\n",
      "  - 23 leaf nodes\n",
      "  - 57 internal nodes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Imports from our project\n",
    "from src.utils.paths import PROJECT_ROOT, get_data_folder\n",
    "from src.utils.ontology_utils import load_ontology  # Still need this to access term names\n",
    "\n",
    "# --- 1. Load Preprocessed Data Artifacts ---\n",
    "# Instead of running preprocessing, we now load the files created by `run_preprocessing.py`.\n",
    "\n",
    "# Hardcoded date for loading the preprocessed files\n",
    "DATE = '2025-10-24'\n",
    "PROCESSED_DATA_DIR = get_data_folder(DATE)\n",
    "\n",
    "print(f\"Loading data from: {PROCESSED_DATA_DIR} for date {DATE}\")\n",
    "\n",
    "# Load the ontology object to get term names for printing\n",
    "cl = load_ontology()\n",
    "\n",
    "# Load DataFrames\n",
    "marginalization_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_marginalization_df.csv\", index_col=0)\n",
    "parent_child_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_parent_child_df.csv\", index_col=0)\n",
    "exclusion_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_exclusion_df.csv\", index_col=0)\n",
    "\n",
    "# Load mapping_dict\n",
    "mapping_dict_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_mapping_dict_df.csv\", index_col=0)\n",
    "# The DataFrame was saved with CL numbers as the index and integer mappings in the first column\n",
    "mapping_dict = pd.Series(mapping_dict_df.iloc[:, 0].values, index=mapping_dict_df.index).to_dict()\n",
    "\n",
    "# Load leaf and internal values\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_leaf_values.pkl\", \"rb\") as fp:\n",
    "    leaf_values = pickle.load(fp)\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_internal_values.pkl\", \"rb\") as fp:\n",
    "    internal_values = pickle.load(fp)\n",
    "\n",
    "print(\"\\nAll data artifacts loaded successfully.\")\n",
    "print(f\"Loaded {len(mapping_dict)} cell types.\")\n",
    "print(f\"  - {len(leaf_values)} leaf nodes\")\n",
    "print(f\"  - {len(internal_values)} internal nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein-coding genes from BioMart...\n",
      "Loaded 23262 protein-coding genes from BioMart\n",
      "Ready to query 80 cell types and 23262 protein-coding genes.\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "import tiledbsoma as soma\n",
    "from tiledbsoma_ml import ExperimentDataset, experiment_dataloader\n",
    "import pandas as pd\n",
    "\n",
    "# Get all cell types from our mapping dict to build the query\n",
    "all_cell_values = list(mapping_dict.keys())\n",
    "\n",
    "# --- Load gene list from BioMart (matching old_reference approach) ---\n",
    "print(\"Loading protein-coding genes from BioMart...\")\n",
    "biomart_path = PROJECT_ROOT / \"hpc_workaround/data/mart_export.txt\"\n",
    "biomart = pd.read_csv(biomart_path)\n",
    "\n",
    "# Filter for protein-coding genes only\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "gene_list = coding_only['Gene stable ID'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(gene_list)} protein-coding genes from BioMart\")\n",
    "\n",
    "# Create the 'value_filter' strings for the query\n",
    "var_value_filter = f\"feature_id in {gene_list}\"\n",
    "obs_value_filter = f'assay == \"10x 3\\' v3\" and is_primary_data == True and cell_type_ontology_term_id in {all_cell_values}'\n",
    "\n",
    "print(f\"Ready to query {len(all_cell_values)} cell types and {len(gene_list)} protein-coding genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the local SOMA database (which is already the homo_sapiens experiment)\n",
    "soma_uri = \"/scratch/sigbio_project_root/sigbio_project25/jingqiao/mccell-single/soma_db_homo_sapiens\"\n",
    "print(f\"Opening local SOMA database at: {soma_uri}\")\n",
    "\n",
    "# Open the experiment directly (it's a SOMAExperiment, not a SOMACollection)\n",
    "experiment = soma.open(soma_uri, mode=\"r\")\n",
    "\n",
    "# Create the ExperimentDataset and DataLoaders using the query filters\n",
    "with experiment.axis_query(\n",
    "    measurement_name=\"RNA\",\n",
    "    obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "    var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    ") as query:\n",
    "    experiment_dataset = ExperimentDataset(\n",
    "        query,\n",
    "        obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "        layer_name=\"raw\",\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        seed=111\n",
    "    )\n",
    "\n",
    "    train_dataset, val_dataset = experiment_dataset.random_split(0.8, 0.2, seed=42)\n",
    "    \n",
    "    # Get cell count before context closes\n",
    "    actual_cell_count = len(experiment_dataset.query_ids.obs_joinids)\n",
    "    print(f'\\nTotal matching cells: {actual_cell_count}')\n",
    "    print(f'Training set size: {len(train_dataset)}')\n",
    "    print(f'Validation set size: {len(val_dataset)}')\n",
    "\n",
    "# Create dataloaders OUTSIDE the context manager to avoid potential issues\n",
    "train_dataloader = experiment_dataloader(train_dataset)\n",
    "val_dataloader = experiment_dataloader(val_dataset)\n",
    "\n",
    "# Show a summary of the loaded train and validation datasets\n",
    "print(\"\\nTrain dataset shape:\", train_dataset.shape)\n",
    "print(\"Validation dataset shape:\", val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DEBUG: Filter strings\n",
      "================================================================================\n",
      "Number of cell types: 80\n",
      "Number of genes: 23262\n",
      "\n",
      "obs_value_filter (first 200 chars):\n",
      "assay == \"10x 3' v3\" and is_primary_data == True and cell_type_ontology_term_id in ['CL:0000233', 'CL:0000895', 'CL:0000900', 'CL:0000904', 'CL:0000905', 'CL:0000910', 'CL:0000912', 'CL:0000913', 'CL:...\n",
      "\n",
      "var_value_filter (first 200 chars):\n",
      "feature_id in ['ENSG00000198888', 'ENSG00000198763', 'ENSG00000198804', 'ENSG00000198712', 'ENSG00000228253', 'ENSG00000198899', 'ENSG00000198938', 'ENSG00000198840', 'ENSG00000212907', 'ENSG000001988...\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DEBUG: Print the actual filter strings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DEBUG: Filter strings\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Number of cell types: {len(all_cell_values)}\")\n",
    "print(f\"Number of genes: {len(gene_list)}\")\n",
    "print(f\"\\nobs_value_filter (first 200 chars):\")\n",
    "print(obs_value_filter[:200] + \"...\")\n",
    "print(f\"\\nvar_value_filter (first 200 chars):\")\n",
    "print(var_value_filter[:200] + \"...\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Setup with Multi-GPU Support ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check for multiple GPUs\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Available GPUs: {num_gpus}\")\n",
    "if num_gpus > 1:\n",
    "    print(f\"GPU names: {[torch.cuda.get_device_name(i) for i in range(num_gpus)]}\")\n",
    "\n",
    "# The input dimension is the number of genes from our dataset object\n",
    "input_dim = train_dataset.shape[1]\n",
    "output_dim = len(leaf_values)  # Model only predicts leaf nodes\n",
    "\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim)\n",
    "\n",
    "# Wrap model with DataParallel for multi-GPU training\n",
    "if num_gpus > 1:\n",
    "    print(f\"\\nüöÄ Using DataParallel with {num_gpus} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    print(f\"   Effective batch size: {256 * num_gpus} (256 per GPU √ó {num_gpus} GPUs)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Single GPU mode\")\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Instantiate the new, correct loss function with all required artifacts\n",
    "loss_fn = MarginalizationLoss(\n",
    "    marginalization_df=marginalization_df,\n",
    "    parent_child_df=parent_child_df,\n",
    "    exclusion_df=exclusion_df,\n",
    "    leaf_values=leaf_values,\n",
    "    internal_values=internal_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\nModel, optimizer, and loss function are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(train_dataset)  # Use ALL training batches (19,665)\n",
    "batch_loss_history = []\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Total training batches: {len(train_dataset):,}\")\n",
    "print(f\"Total training cells: ~{len(train_dataset) * 256:,}\")\n",
    "print(f\"Batches per epoch: {batches_per_epoch:,} (100% of data)\")\n",
    "print(f\"Number of epochs: {num_epochs}\")\n",
    "print(f\"Total batches to process: {batches_per_epoch * num_epochs:,}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"‚è±Ô∏è  Estimating ~15-20 hours per epoch on single GPU\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"‚ö° With {torch.cuda.device_count()} GPUs, expecting ~{15//torch.cuda.device_count()}-{20//torch.cuda.device_count()} hours per epoch\")\n",
    "print()\n",
    "\n",
    "epoch_times = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'EPOCH {epoch + 1}/{num_epochs}')\n",
    "    print(f'{\"=\"*80}')\n",
    "    \n",
    "    epoch_losses = []\n",
    "    batch_count = 0\n",
    "\n",
    "    for i, (X_batch, obs_batch) in enumerate(train_dataloader):\n",
    "        if i >= batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        # Data preparation\n",
    "        X_batch = torch.from_numpy(X_batch).float()\n",
    "        X_batch = torch.log1p(X_batch)  # Log-transform gene expression\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "        y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch)\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        batch_loss_history.append(total_loss.item())\n",
    "        epoch_losses.append(total_loss.item())\n",
    "        batch_count += 1\n",
    "        \n",
    "        # Progress updates every 1000 batches\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            elapsed = time.time() - epoch_start_time\n",
    "            batches_remaining = batches_per_epoch - (i + 1)\n",
    "            time_per_batch = elapsed / (i + 1)\n",
    "            eta_seconds = batches_remaining * time_per_batch\n",
    "            eta_hours = eta_seconds / 3600\n",
    "            \n",
    "            avg_loss_recent = sum(epoch_losses[-1000:]) / len(epoch_losses[-1000:])\n",
    "            print(f'  [Batch {i+1:5d}/{batches_per_epoch}] '\n",
    "                  f'Loss: {total_loss.item():.4f} (avg: {avg_loss_recent:.4f}) | '\n",
    "                  f'Elapsed: {elapsed/3600:.2f}h | ETA: {eta_hours:.2f}h')\n",
    "\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    epoch_times.append(epoch_time)\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    \n",
    "    print(f'\\n--- Epoch {epoch + 1} Summary ---')\n",
    "    print(f'  Time: {epoch_time/3600:.2f} hours ({epoch_time/60:.1f} minutes)')\n",
    "    print(f'  Batches processed: {batch_count:,}')\n",
    "    print(f'  Average loss: {avg_epoch_loss:.4f}')\n",
    "    print(f'  Final loss: {epoch_losses[-1]:.4f}')\n",
    "    \n",
    "    if len(epoch_times) > 1:\n",
    "        avg_time = sum(epoch_times) / len(epoch_times)\n",
    "        remaining_epochs = num_epochs - (epoch + 1)\n",
    "        total_eta = (avg_time * remaining_epochs) / 3600\n",
    "        print(f'  Estimated time remaining: {total_eta:.1f} hours')\n",
    "\n",
    "print('\\n' + '='*80)\n",
    "print('TRAINING COMPLETE')\n",
    "print('='*80)\n",
    "print(f'Total time: {sum(epoch_times)/3600:.2f} hours')\n",
    "print(f'Average time per epoch: {sum(epoch_times)/len(epoch_times)/3600:.2f} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create directory for saving models in data folder\n",
    "save_dir = PROJECT_ROOT / \"data\" / \"saved_models\"\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Save directory: {save_dir}\")\n",
    "\n",
    "# Generate timestamp for this training run\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "model_name = f\"blood_cell_model_{timestamp}\"\n",
    "\n",
    "# Save the model weights\n",
    "model_path = save_dir / f\"{model_name}.pt\"\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SAVING MODEL\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# If using DataParallel, need to save the underlying module\n",
    "if isinstance(model, torch.nn.DataParallel):\n",
    "    state_dict_to_save = model.module.state_dict()\n",
    "    print(\"Saving DataParallel model (extracting module.state_dict())\")\n",
    "else:\n",
    "    state_dict_to_save = model.state_dict()\n",
    "    print(\"Saving single GPU model\")\n",
    "\n",
    "# Save full checkpoint with training state\n",
    "checkpoint = {\n",
    "    'model_state_dict': state_dict_to_save,\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': num_epochs,\n",
    "    'batch_loss_history': batch_loss_history,\n",
    "    'epoch_times': epoch_times,\n",
    "    'num_gpus': torch.cuda.device_count(),\n",
    "    'input_dim': input_dim,\n",
    "    'output_dim': output_dim,\n",
    "    'date_preprocessed': DATE,\n",
    "    'total_training_cells': len(train_dataset) * 256,\n",
    "    'total_batches_processed': len(batch_loss_history),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, model_path)\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "print(f\"  - Model architecture: SimpleNN({input_dim} -> {output_dim})\")\n",
    "print(f\"  - Training epochs: {num_epochs}\")\n",
    "print(f\"  - Total batches: {len(batch_loss_history):,}\")\n",
    "print(f\"  - Final loss: {batch_loss_history[-1]:.4f}\")\n",
    "print(f\"  - Total training time: {sum(epoch_times)/3600:.2f} hours\")\n",
    "\n",
    "# Also save just the model weights (smaller file, easier to load)\n",
    "weights_path = save_dir / f\"{model_name}_weights_only.pt\"\n",
    "torch.save(state_dict_to_save, weights_path)\n",
    "print(f\"\\nWeights-only file saved to: {weights_path}\")\n",
    "print(f\"  - File size: ~{os.path.getsize(weights_path) / 1e6:.1f} MB\")\n",
    "\n",
    "# Save training history as CSV for easy plotting later\n",
    "history_df = pd.DataFrame({\n",
    "    'batch': range(1, len(batch_loss_history) + 1),\n",
    "    'loss': batch_loss_history\n",
    "})\n",
    "history_path = save_dir / f\"{model_name}_training_history.csv\"\n",
    "history_df.to_csv(history_path, index=False)\n",
    "print(f\"\\nTraining history saved to: {history_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"To load this model later:\")\n",
    "print(f\"  checkpoint = torch.load('{model_path}')\")\n",
    "print(f\"  model.load_state_dict(checkpoint['model_state_dict'])\")\n",
    "print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Plot Training Loss ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batch_loss_history, alpha=0.6, linewidth=0.8)\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Training Loss per Batch')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Moving average for smoother trend\n",
    "window_size = 20\n",
    "if len(batch_loss_history) >= window_size:\n",
    "    moving_avg = np.convolve(batch_loss_history, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(range(window_size-1, len(batch_loss_history)), moving_avg, \n",
    "             color='red', linewidth=2, label=f'{window_size}-batch moving avg')\n",
    "    plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot loss by epoch (if multiple epochs)\n",
    "if num_epochs > 1:\n",
    "    epoch_losses = [batch_loss_history[i*batches_per_epoch:(i+1)*batches_per_epoch] \n",
    "                   for i in range(num_epochs)]\n",
    "    for i, losses in enumerate(epoch_losses):\n",
    "        plt.plot(losses, label=f'Epoch {i+1}', alpha=0.7)\n",
    "    plt.xlabel('Batch (within epoch)')\n",
    "    plt.ylabel('Total Loss')\n",
    "    plt.title('Training Loss by Epoch')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Run multiple epochs\\nto see comparison', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  Initial loss: {batch_loss_history[0]:.4f}\")\n",
    "print(f\"  Final loss: {batch_loss_history[-1]:.4f}\")\n",
    "print(f\"  Loss reduction: {(1 - batch_loss_history[-1]/batch_loss_history[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport glob\n\n# --- Load the most recent saved model ---\nsave_dir = PROJECT_ROOT / \"data\" / \"saved_models\"\nmodel_files = sorted(glob.glob(str(save_dir / \"blood_cell_model_*.pt\")))\n\nif not model_files:\n    print(\"ERROR: No saved model found in data/saved_models/\")\n    print(\"Please run training first (cell-6 and cell-7)\")\nelse:\n    latest_model = model_files[-1]  # Get most recent\n    print(f\"Loading model from: {latest_model}\")\n    \n    # Load checkpoint\n    checkpoint = torch.load(latest_model)\n    \n    # Create fresh model with same architecture\n    input_dim = checkpoint['input_dim']\n    output_dim = checkpoint['output_dim']\n    \n    validation_model = SimpleNN(input_dim=input_dim, output_dim=output_dim)\n    \n    # Handle DataParallel if needed\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 1:\n        validation_model = torch.nn.DataParallel(validation_model)\n        validation_model.module.load_state_dict(checkpoint['model_state_dict'])\n        print(f\"Loaded model with DataParallel ({num_gpus} GPUs)\")\n    else:\n        validation_model.load_state_dict(checkpoint['model_state_dict'])\n        print(\"Loaded model (single GPU)\")\n    \n    validation_model = validation_model.to(device)\n    validation_model.eval()\n    \n    print(f\"Model info:\")\n    print(f\"  - Trained epochs: {checkpoint.get('num_epochs', 'unknown')}\")\n    print(f\"  - Total batches: {checkpoint.get('total_batches_processed', 'unknown'):,}\")\n    print(f\"  - Architecture: SimpleNN({input_dim} -> {output_dim})\")\n    print()\n\n    # --- Validate the Model (LEAF NODES ONLY) ---\n    print(\"=\"*60)\n    print(\"VALIDATION (LEAF NODES ONLY)\")\n    print(\"=\"*60)\n\n    val_batches = 50\n\n    val_total_losses = []\n    val_leaf_losses = []\n    val_parent_losses = []\n    all_predictions = []\n    all_labels = []\n\n    leaf_indices_set = {mapping_dict[cid] for cid in leaf_values}\n    total_samples = 0\n    leaf_samples = 0\n\n    with torch.no_grad():\n        for i, (X_batch, obs_batch) in enumerate(val_dataloader):\n            if i >= val_batches:\n                break\n\n            # Data preparation\n            X_batch = torch.from_numpy(X_batch).float()\n            X_batch = torch.log1p(X_batch)\n            X_batch = X_batch.to(device)\n\n            label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n            y_batch = torch.tensor([mapping_dict[term] for term in label_strings],\n                                  device=device, dtype=torch.long)\n\n            total_samples += len(y_batch)\n\n            # FILTER: Only keep samples with LEAF node labels\n            is_leaf = torch.tensor([y.item() in leaf_indices_set for y in y_batch], device=device)\n\n            if is_leaf.sum() == 0:\n                continue  # Skip batches with no leaf samples\n\n            X_batch_leaf = X_batch[is_leaf]\n            y_batch_leaf = y_batch[is_leaf]\n            leaf_samples += len(y_batch_leaf)\n\n            # Forward pass\n            outputs = validation_model(X_batch_leaf)\n            total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch_leaf)\n\n            val_total_losses.append(total_loss.item())\n            val_leaf_losses.append(loss_leafs.item())\n            val_parent_losses.append(loss_parents.item())\n\n            # Get predictions (argmax of logits)\n            predictions = torch.argmax(outputs, dim=1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(y_batch_leaf.cpu().numpy())\n\n    # Calculate metrics\n    avg_val_loss = np.mean(val_total_losses)\n    avg_leaf_loss = np.mean(val_leaf_losses)\n    avg_parent_loss = np.mean(val_parent_losses)\n\n    # Accuracy (only meaningful for leaf nodes)\n    all_predictions = np.array(all_predictions)\n    all_labels = np.array(all_labels)\n    accuracy = (all_predictions == all_labels).mean()\n\n    print(f\"\\nValidation Results (LEAF-LABELED SAMPLES ONLY):\")\n    print(f\"  Total samples processed: {total_samples}\")\n    print(f\"  Leaf-labeled samples: {leaf_samples} ({leaf_samples/total_samples*100:.1f}%)\")\n    print(f\"  Internal-labeled samples (skipped): {total_samples - leaf_samples}\")\n    print(f\"\\n  Average Total Loss: {avg_val_loss:.4f}\")\n    print(f\"  Average Leaf Loss:  {avg_leaf_loss:.4f}\")\n    print(f\"  Average Parent Loss: {avg_parent_loss:.4f}\")\n    print(f\"  Leaf Accuracy: {accuracy*100:.2f}%\")\n    print(f\"\\nNote: This validation only tests the model on leaf node predictions.\")\n    print(f\"      Internal node labels are excluded from evaluation.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "McCell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}