{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/jzhao/dev/Welch-lab/McCell/data/processed for date 2025-10-17\n",
      "Loading cached ontology from /Users/jzhao/dev/Welch-lab/McCell/data/processed/ontology.pkl...\n",
      "Ontology loaded successfully.\n",
      "\n",
      "All data artifacts loaded successfully.\n",
      "Loaded 141 cell types.\n",
      "  - 41 leaf nodes\n",
      "  - 100 internal nodes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Imports from our project\n",
    "from src.utils.paths import PROJECT_ROOT\n",
    "from src.utils.ontology_utils import load_ontology  # Still need this to access term names\n",
    "\n",
    "# --- 1. Load Preprocessed Data Artifacts ---\n",
    "# Instead of running preprocessing, we now load the files created by `run_preprocessing.py`.\n",
    "\n",
    "# Hardcoded date for loading the preprocessed files\n",
    "DATE = '2025-10-17'\n",
    "PROCESSED_DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "print(f\"Loading data from: {PROCESSED_DATA_DIR} for date {DATE}\")\n",
    "\n",
    "# Load the ontology object to get term names for printing\n",
    "cl = load_ontology()\n",
    "\n",
    "# Load DataFrames\n",
    "marginalization_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_marginalization_df.csv\", index_col=0)\n",
    "parent_child_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_parent_child_df.csv\", index_col=0)\n",
    "exclusion_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_exclusion_df.csv\", index_col=0)\n",
    "\n",
    "# Load mapping_dict\n",
    "mapping_dict_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_mapping_dict_df.csv\", index_col=0)\n",
    "# The DataFrame was saved with CL numbers as the index and integer mappings in the first column\n",
    "mapping_dict = pd.Series(mapping_dict_df.iloc[:, 0].values, index=mapping_dict_df.index).to_dict()\n",
    "\n",
    "# Load leaf and internal values\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_leaf_values.pkl\", \"rb\") as fp:\n",
    "    leaf_values = pickle.load(fp)\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_internal_values.pkl\", \"rb\") as fp:\n",
    "    internal_values = pickle.load(fp)\n",
    "\n",
    "print(\"\\nAll data artifacts loaded successfully.\")\n",
    "print(f\"Loaded {len(mapping_dict)} cell types.\")\n",
    "print(f\"  - {len(leaf_values)} leaf nodes\")\n",
    "print(f\"  - {len(internal_values)} internal nodes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading protein-coding genes from BioMart...\n",
      "Loaded 23262 protein-coding genes from BioMart\n",
      "Ready to query 141 cell types and 23262 protein-coding genes.\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "import tiledbsoma as soma\n",
    "from tiledbsoma_ml import ExperimentDataset, experiment_dataloader\n",
    "import pandas as pd\n",
    "\n",
    "# Get all cell types from our mapping dict to build the query\n",
    "all_cell_values = list(mapping_dict.keys())\n",
    "\n",
    "# --- Load gene list from BioMart (matching old_reference approach) ---\n",
    "print(\"Loading protein-coding genes from BioMart...\")\n",
    "biomart_path = PROJECT_ROOT / \"hpc_workaround/data/mart_export.txt\"\n",
    "biomart = pd.read_csv(biomart_path)\n",
    "\n",
    "# Filter for protein-coding genes only\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "gene_list = coding_only['Gene stable ID'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(gene_list)} protein-coding genes from BioMart\")\n",
    "\n",
    "# Create the 'value_filter' strings for the query\n",
    "var_value_filter = f\"feature_id in {gene_list}\"\n",
    "obs_value_filter = f\"assay == '10x 3\\\\' v3' and is_primary_data == True and cell_type_ontology_term_id in {all_cell_values}\"\n",
    "\n",
    "print(f\"Ready to query {len(all_cell_values)} cell types and {len(gene_list)} protein-coding genes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the local SOMA database (which is already the homo_sapiens experiment)\n",
    "soma_uri = \"/scratch/sigbio_project_root/sigbio_project25/jingqiao/mccell-single/soma_db_homo_sapiens\"\n",
    "print(f\"Opening local SOMA database at: {soma_uri}\")\n",
    "\n",
    "# Open the experiment directly (it's a SOMAExperiment, not a SOMACollection)\n",
    "experiment = soma.open(soma_uri, mode=\"r\")\n",
    "\n",
    "# Create the ExperimentDataset and DataLoaders using the query filters\n",
    "with experiment.axis_query(\n",
    "    measurement_name=\"RNA\",\n",
    "    obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "    var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    ") as query:\n",
    "    experiment_dataset = ExperimentDataset(\n",
    "        query,\n",
    "        obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "        layer_name=\"raw\",\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        seed=111\n",
    "    )\n",
    "\n",
    "    train_dataset, val_dataset = experiment_dataset.random_split([0.8, 0.2], seed=42)\n",
    "\n",
    "    print(f'\\nTotal matching cells: {len(experiment_dataset)}')\n",
    "    print(f'Training set size: {len(train_dataset)}')\n",
    "    print(f'Validation set size: {len(val_dataset)}')\n",
    "\n",
    "    train_dataloader = experiment_dataloader(train_dataset)\n",
    "    val_dataloader = experiment_dataloader(val_dataset)\n",
    "\n",
    "# Show a summary of the loaded train and validation datasets\n",
    "print(\"\\nTrain dataset shape:\", train_dataset.shape)\n",
    "print(\"Validation dataset shape:\", val_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Setup ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# The input dimension is the number of genes from our dataset object\n",
    "input_dim = train_dataset.shape[1]\n",
    "output_dim = len(leaf_values)  # Model only predicts leaf nodes\n",
    "\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Instantiate the new, correct loss function with all required artifacts\n",
    "loss_fn = MarginalizationLoss(\n",
    "    marginalization_df=marginalization_df,\n",
    "    parent_child_df=parent_child_df,\n",
    "    exclusion_df=exclusion_df,\n",
    "    leaf_values=leaf_values,\n",
    "    internal_values=internal_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Model, optimizer, and loss function are ready.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "num_epochs = 2\nbatches_per_epoch = 200\nbatch_loss_history = []\n\nprint(f\"\\nStarting training for {num_epochs} epochs ({batches_per_epoch} batches each)...\")\nfor epoch in range(num_epochs):\n    model.train()\n    print(f'\\n--- Epoch {epoch + 1} ---')\n\n    for i, (X_batch, obs_batch) in enumerate(train_dataloader):\n        if i >= batches_per_epoch:\n            break\n\n        # Data preparation\n        X_batch = torch.from_numpy(X_batch).float()\n        X_batch = torch.log1p(X_batch)  # Log-transform gene expression\n        X_batch = X_batch.to(device)\n        \n        label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n        y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n\n        # Training step\n        optimizer.zero_grad()\n        outputs = model(X_batch)\n        total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch)\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n        optimizer.step()\n\n        # Logging\n        batch_loss_history.append(total_loss.item())\n        if (i + 1) % 50 == 0:\n            print(f'  [Batch {i + 1:3d}] Total Loss: {total_loss.item():.4f} (Leaf: {loss_leafs.item():.4f}, Parent: {loss_parents.item():.4f})')\n\nprint('\\nFinished Training.')\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.92.136.210\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostbyname(\"cellxgene-census-public-us-west-2.s3.us-west-2.amazonaws.com\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "McCell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}