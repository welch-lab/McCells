{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from src.utils.ontology_utils import load_ontology\n",
    "from src.data_pipeline.data_loader import load_filtered_cell_metadata\n",
    "from src.data_pipeline.preprocess_ontology import preprocess_data_ontology\n",
    "from src.utils.paths import PROJECT_ROOT\n",
    "from src.utils.ontology_utils import get_sub_DAG\n",
    "\n",
    "# 1. Load the cached ontology object\n",
    "cl = load_ontology()\n",
    "\n",
    "# Define the root of the ontology subgraph to be processed\n",
    "root_cl_id = 'CL:0000988'  # hematopoietic cell\n",
    "\n",
    "# 2. Load filtered cell metadata from CellXGene Census\n",
    "# This step can take a few minutes\n",
    "cell_obs_metadata = load_filtered_cell_metadata(cl, root_cl_id=root_cl_id)\n",
    "\n",
    "# 3. Preprocess the ontology and cell data\n",
    "target_column = 'cell_type_ontology_term_id'\n",
    "\n",
    "mapping_dict, leaf_values, internal_values, ontology_df, cell_children_mask = preprocess_data_ontology(\n",
    "        cl, cell_obs_metadata, target_column,\n",
    "        upper_limit=root_cl_id,\n",
    "        cl_only=True, include_leafs=False\n",
    "    )\n",
    "\n",
    "reverse_mapping_dict = {v: k for k, v in mapping_dict.items()}\n",
    "\n",
    "print(f'Loaded {len(mapping_dict)} cell types.')\n",
    "print(\"Cell children mask shape:\", cell_children_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellxgene_census\n",
    "import cellxgene_census.experimental.ml as census_ml\n",
    "import tiledbsoma as soma\n",
    "\n",
    "# Get the list of all cell types from the preprocessing step\n",
    "all_cell_values = list(mapping_dict.keys())\n",
    "\n",
    "# We need a list of genes to select from the census.\n",
    "# For this example, we'll query the top 2000 highly variable genes\n",
    "# as a stand-in for the full gene list from the old notebook.\n",
    "print(\"Finding highly variable genes...\")\n",
    "with cellxgene_census.open_soma() as census:\n",
    "    highly_variable_genes = (\n",
    "        census[\"census_data\"][\"homo_sapiens\"]\n",
    "        .ms[\"RNA\"]\n",
    "        .var.read(\n",
    "            column_names=[\"feature_id\", \"highly_variable\"]\n",
    "        )\n",
    "        .concat()\n",
    "        .to_pandas()\n",
    "        .query(\"highly_variable == True\")\n",
    "    )\n",
    "top_2000_genes = highly_variable_genes.head(2000)[\'feature_id\'].tolist()\n",
    "\n",
    "    # Create filters for the data query\n",
    "    var_value_filter = f\"feature_id in {top_2000_genes}\"\n",
    "    obs_value_filter = f\"cell_type_ontology_term_id in {all_cell_values}\"\n",
    "\n",
    "    print(f\"Querying {len(all_cell_values)} cell types and {len(top_2000_genes)} genes.\")\n",
    "\n",
    "    # Create the data pipe for streaming data\n",
    "    # This connects to the census and sets up the query\n",
    "    experiment_datapipe = census_ml.pytorch.ExperimentDataPipe(\n",
    "        census[\"census_data\"][\"homo_sapiens\"],\n",
    "        measurement_name=\"RNA\",\n",
    "        X_name=\"raw\",\n",
    "        obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "        var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    "        obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "# Create train/validation split and dataloaders\n",
    "train_datapipe, val_datapipe = experiment_datapipe.random_split(weights={"train": 0.8, "val": 0.2}, seed=42)\n",
    "train_dataloader = census_ml.experiment_dataloader(train_datapipe)\n",
    "val_dataloader = census_ml.experiment_dataloader(val_datapipe)\n",
    "\n",
    "print(\"\\nDataLoaders are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "\n",
    "# --- 1. Setup ---
",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get model dimensions from the data pipe and preprocessed data\n",
    "input_dim = experiment_datapipe.shape[1]\n",
    "output_dim = len(leaf_values)\n",
    "\n",
    "# Instantiate the model, loss, and optimizer\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# The loss function needs the ontology_df and other info from the preprocessing cell\n",
    "loss_fn = MarginalizationLoss(\n",
    "    ontology_df=ontology_df,\n",
    "    leaf_values=leaf_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    "    )\n",
    "\n",
    "# --- 2. Training Loop ---
",
    "num_epochs = 5 # Using 5 for demonstration, can be increased\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_leaf_loss = 0.0\n",
    "    running_parent_loss = 0.0\n",
    "    \n",
    "    # The dataloader streams data from the census\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        X_batch, y_batch_meta = batch\n",
    "        \n",
    "        # Get data and labels\n",
    "        X_batch = X_batch.float().to(device)\n",
    "        # Map the census-encoded labels (ontology term IDs) to our internal integer encoding\n",
    "        y_batch = torch.tensor([mapping_dict[term] for term in y_batch_meta[:, 0]], device=device, dtype=torch.long)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # Loss calculation\n",
    "        total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += total_loss.item()\n",
    "        running_leaf_loss += loss_leafs.item()\n",
    "        running_parent_loss += loss_parents.item()\n",
    "        if i % 100 == 99: # Print every 100 batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f} (Leaf: {running_leaf_loss / 100:.3f}, Parent: {running_parent_loss / 100:.3f})')\n",
    "            running_loss = 0.0\n",
    "            running_leaf_loss = 0.0\n",
    "            running_parent_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "McCell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}