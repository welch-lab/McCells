{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4e25ff",
   "metadata": {},
   "source": [
    "# McCell Model Data Streaming From Disk\n",
    "\n",
    "Model the CellxGene Census data by streaming the data from disk, to save on memory issues. Based on the tutorial from https://chanzuckerberg.github.io/cellxgene-census/notebooks/experimental/pytorch.html\n",
    "\n",
    "Need to run ```mccell_preprocess_from_disk.ipynb``` first, which saves some preprocessing files this program loads and uses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b822fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cellxgene_census\n",
    "import cellxgene_census.experimental.ml as census_ml\n",
    "import tiledbsoma as soma\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch import tensor\n",
    "\n",
    "from torcheval.metrics.functional import multilabel_accuracy\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(style='whitegrid')\n",
    "sns.set_context(context='notebook')\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc(\n",
    "    'axes',\n",
    "    labelweight='bold',\n",
    "    labelsize='large',\n",
    "    titleweight='bold',\n",
    "    titlesize=9,\n",
    "    linewidth=4\n",
    "    )\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9355266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dba2fb",
   "metadata": {},
   "source": [
    "## Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ed289f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(marginalization_dict,num_epochs,save_title=None):\n",
    "    fig, ax = plt.subplots(4,2,figsize=(9,12))\n",
    "    \n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_train_leaf_hist'], \n",
    "                    ax = ax[0,0],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_val_leaf_hist'], \n",
    "                    ax = ax[0,0],color='mediumslateblue',label='Validation')\n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_train_internal_hist'],\n",
    "                   ax = ax[0,1],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['accuracy_val_internal_hist'],\n",
    "                   ax = ax[0,1],color='mediumslateblue',label='Validation')\n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_hist'], \n",
    "                    ax = ax[1,0],color='lightcoral')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_hist'], \n",
    "                    ax = ax[1,0],color='mediumslateblue')\n",
    "    \n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_leaf_hist'], \n",
    "                    ax = ax[1,1],color='lightcoral',marker='X',label='Train Leafs')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_internal_hist'], \n",
    "                    ax = ax[1,1],color='lightcoral',marker='o',label='Train Internal')\n",
    "    #sns.scatterplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_train_internal_hist'], \n",
    "    #                ax = ax[1,0],color='lightcoral',marker='v',label='Train Internal')\n",
    "\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_leaf_hist'], \n",
    "                    ax = ax[2,0],color='mediumslateblue',marker='X',label='Val Leafs')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_internal_hist'], \n",
    "                    ax = ax[2,0],color='mediumslateblue',marker='o',label='Val Internal')\n",
    "    #sns.scatterplot(x=range(1,num_epochs+1),y=marginalization_dict['loss_val_internal_hist'], \n",
    "    #                ax = ax[1,1],color='mediumslateblue',marker='v',label='Val Internal')\n",
    "\n",
    "\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_train_leaf'], \n",
    "                    ax = ax[2,1],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_val_leaf'], \n",
    "                    ax = ax[2,1],color='mediumslateblue',label='Validation')\n",
    "\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_train_internal'], \n",
    "                    ax = ax[3,0],color='lightcoral',label='Train')\n",
    "    sns.lineplot(x=range(1,num_epochs+1),y=marginalization_dict['f1_score_val_internal'], \n",
    "                    ax = ax[3,0],color='mediumslateblue',label='Validation')\n",
    "\n",
    "    \n",
    "    ax[0,0].set_xlabel('Epoch')\n",
    "    ax[0,1].set_xlabel('Epoch')\n",
    "    ax[1,0].set_xlabel('Epoch')\n",
    "    ax[1,1].set_xlabel('Epoch')\n",
    "    ax[2,0].set_xlabel('Epoch')\n",
    "    ax[2,1].set_xlabel('Epoch')\n",
    "    ax[3,0].set_xlabel('Epoch')\n",
    "\n",
    "\n",
    "    ax[0,0].set_ylabel('Leaf Accuracy')\n",
    "    ax[0,1].set_ylabel('Internal Accuracy')\n",
    "    ax[1,0].set_ylabel('Total Loss')\n",
    "    ax[1,1].set_ylabel('Training Loss')\n",
    "    ax[2,0].set_ylabel('Validation Loss')\n",
    "    ax[2,1].set_ylabel('Leaf F1 Score')\n",
    "    ax[3,0].set_ylabel('Internal F1 Score')\n",
    "\n",
    "\n",
    "    # set the boundary for the accuracy plots\n",
    "    #ax[0,1].set_ylim((50,100))\n",
    "    \n",
    "    # turn off the axis for subplot 2,1\n",
    "    ax[3,1].axis('off')\n",
    "    \n",
    "    if save_title:\n",
    "        plt.savefig(save_title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e491b4",
   "metadata": {},
   "source": [
    "## Load the Saved Outputs from McCell_preprocessing\n",
    "\n",
    "The preproccessing and modeling here should be run on the **same dataset**. If not, there might be differences in the ordering of cells the would nullify this model. \n",
    "\n",
    "- cell_parent_mask\n",
    "- Mapping_dict\n",
    "- Ontology_df\n",
    "- Internal_values\n",
    "- leaf_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "531a2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing outputs are normally stored on Turbo\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell/preprocessing_outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6c952ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the date in yyyy-mm-dd format\n",
    "date = '2024-10-11'\n",
    "# date = '2024-10-14'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea04d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ontology_df_name = date + '_ontology_df.csv'\n",
    "ontology_df = pd.read_csv(ontology_df_name,index_col=0)\n",
    "\n",
    "mapping_dict_name = date + '_mapping_dict_df.csv'\n",
    "mapping_dict_df = pd.read_csv(mapping_dict_name,index_col=0)\n",
    "mapping_dict = mapping_dict_df.T.to_dict('list')\n",
    "# the values are stored as a list. convert to single value\n",
    "for key, value in mapping_dict.items():\n",
    "    mapping_dict[key] = value[0]\n",
    "\n",
    "leaf_values_name = date + '_leaf_values'\n",
    "with open(leaf_values_name,'rb') as fp:\n",
    "    leaf_values = pickle.load(fp)\n",
    "\n",
    "internal_values_name = date + '_internal_values'\n",
    "with open(internal_values_name,'rb') as fp:\n",
    "    internal_values = pickle.load(fp)\n",
    "\n",
    "cell_parent_mask_name = date + '_cell_parent_mask.pt'\n",
    "cell_parent_mask = torch.load(cell_parent_mask_name)\n",
    "\n",
    "valid_cell_types = date + \"_valid_cell_types.txt\"\n",
    "with open(valid_cell_types,'rb') as fp:\n",
    "    cell_type_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caceac8",
   "metadata": {},
   "source": [
    "## Build the Experiment and the DataPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5274b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene and cell type info stored on Turbo\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e2d95a",
   "metadata": {},
   "source": [
    "First, let's load the gene list and cell type list that we want from the Census. Then we construct the ```var_val_filter``` and ```obs_val_filter``` for querying the census."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065f9e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gene list\n",
    "biomart = pd.read_csv('mart_export.txt')\n",
    "\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "\n",
    "gene_list = coding_only['Gene stable ID'].to_list()\n",
    "\n",
    "var_val_filter = '''feature_id in {}'''.format(gene_list)\n",
    "\n",
    "obs_val_filter = '''assay == \"10x 3\\' v3\" and is_primary_data == True and cell_type_ontology_term_id in {}'''.format(cell_type_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0f26d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#organism = \"Homo sapiens\"\n",
    "col_names = {\"obs\": [\"cell_type_ontology_term_id\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "192a33e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the Census and create the Experiment\n",
    "# census = cellxgene_census.open_soma(uri = \"/scratch/welchjd_root/welchjd99/fujoshua/soma\")\n",
    "census = cellxgene_census.open_soma(uri = \"/scratch/sigbio_project_root/sigbio_project25/jrepucci/mccell_data/soma\")\n",
    "\n",
    "experiment = census[\"census_data\"][\"homo_sapiens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf77354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(self, experiment: tiledbsoma._experiment.Experiment, measurement_name: str = 'RNA', X_name: str = 'raw', obs_query: Optional[somacore.query.axis.AxisQuery] = None, var_query: Optional[somacore.query.axis.AxisQuery] = None, obs_column_names: Sequence[str] = (), batch_size: int = 1, shuffle: bool = True, seed: Optional[int] = None, return_sparse_X: bool = False, soma_chunk_size: Optional[int] = 64, use_eager_fetch: bool = True, encoders: Optional[List[cellxgene_census.experimental.ml.encoders.Encoder]] = None, shuffle_chunk_count: Optional[int] = 2000) -> None\n"
     ]
    }
   ],
   "source": [
    "# import required modules\n",
    "import inspect\n",
    "import collections\n",
    "\n",
    "# use signature()\n",
    "print(inspect.signature(census_ml.ExperimentDataPipe.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437e76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "soma_chunk_size = 64\n",
    "\n",
    "experiment_datapipe = census_ml.pytorch.ExperimentDataPipe(\n",
    "    experiment,\n",
    "    measurement_name=\"RNA\",\n",
    "    X_name=\"raw\",\n",
    "    obs_query=soma.AxisQuery(value_filter=obs_val_filter),\n",
    "    var_query=soma.AxisQuery(value_filter=var_val_filter),\n",
    "    obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=200,\n",
    "#     return_sparse_X=True,\n",
    "    shuffle_chunk_count=8000,\n",
    "    soma_chunk_size=soma_chunk_size,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54f2e94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13735353, 19966)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_datapipe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195167e",
   "metadata": {},
   "source": [
    "Split the datapipe into Train and Test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d4fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percent of the data we'll actually use for training.\n",
    "# We can lower it to a value like 0.1 to use a smaller subset\n",
    "# while debugging\n",
    "sub_percent = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1373237",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percent = 0.8\n",
    "val_percent = 0.2\n",
    "\n",
    "# if you give random_split only two values, it normalizes those to the whole dataset, \n",
    "# i.e. if you give it 0.12 and 0.08 (in an attempt to train a subset of the dataset), \n",
    "# you get a 60/40 traing/validation split\n",
    "big_train_datapipe, big_val_datapipe = experiment_datapipe.random_split(weights={\"train\": train_percent, \"val\": val_percent},\n",
    "                                                                seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011c2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use headers to subset data for training\n",
    "# When using the full census, it may make sense to comment these\n",
    "# two lines out and rename the big_train_datapipe/big_val_datapipe variables\n",
    "# to train_datapipe/val_datapipe\n",
    "train_datapipe = big_train_datapipe.header(int((experiment_datapipe.shape[0] * train_percent * sub_percent) / batch_size))\n",
    "val_datapipe = big_val_datapipe.header(int((experiment_datapipe.shape[0] * val_percent * sub_percent) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba27c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train size\", int((experiment_datapipe.shape[0] * train_percent * sub_percent)))\n",
    "print(\"val size\", int((experiment_datapipe.shape[0] * val_percent * sub_percent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325fe3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training batches is {:.2f}'.format(int((experiment_datapipe.shape[0]*train_percent*sub_percent)/batch_size)))\n",
    "print('Number of validation batches is {:.2f}'.format(int((experiment_datapipe.shape[0]*val_percent*sub_percent)/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89f9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"extra percent:\", sub_percent)\n",
    "print(\"train percent:\", train_percent)\n",
    "print(\"val percent:\", val_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f5e6e",
   "metadata": {},
   "source": [
    "Build the dataloaders for the train and test splits. We don't use PyTorch ```DataLoader``` directly because the ```ExperimentDataPipe``` already deals with the necessary parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b58334",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = census_ml.experiment_dataloader(train_datapipe)\n",
    "val_dataloader = census_ml.experiment_dataloader(val_datapipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_batch = time.time()\n",
    "for batch in val_dataloader:\n",
    "    \n",
    "    X_batch, y_batch = batch\n",
    "    \n",
    "    print('running time', (time.time()-start_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_batch = time.time()\n",
    "for batch in train_dataloader:\n",
    "    \n",
    "    X_batch, y_batch = batch\n",
    "    \n",
    "    print('running time', (time.time()-start_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ada2e8",
   "metadata": {},
   "source": [
    "## Build Neural Network Classifier\n",
    "\n",
    "First, we need to select and define the input and output dimensions from the data. The number of neurons for the hidden nodes is defined manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07043911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features (len of X cols)\n",
    "# select the number of gene columns\n",
    "input_dim = experiment_datapipe.shape[1]#X_train.size(dim=1) #adata.X.shape[1] \n",
    "\n",
    "# number of neurons for hidden layers\n",
    "hidden_layer_1 = 2048\n",
    "hidden_layer_2 = 1024\n",
    "hidden_layer_3 = 256\n",
    "\n",
    "# number of leaf classes (unique of y that are greater than or equal to 0)\n",
    "output_dim = len(leaf_values)\n",
    "\n",
    "print(input_dim,hidden_layer_1,hidden_layer_2,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c374fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value to multiply leaf loss by. We want the neural network to care more\n",
    "# about leaf loss than parent loss, so this value is greater than 1\n",
    "leaf_weight = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83fe79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_layer_1)\n",
    "        self.linear2 = nn.Linear(hidden_layer_1,hidden_layer_2)\n",
    "        self.linear25 = nn.Linear(hidden_layer_2,hidden_layer_3)\n",
    "        self.linear3 = nn.Linear(hidden_layer_3,output_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_layer_1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_layer_2)\n",
    "        self.bn25 = nn.BatchNorm1d(hidden_layer_3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear25(x)\n",
    "        x = self.bn25(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "    def get_last_layer(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear25(x)\n",
    "        x = self.bn25(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):nn.init.kaiming_normal_(m.weight, nonlinearity='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cf977d",
   "metadata": {},
   "source": [
    "## Functions for dealing with cell ontology and loss calculations\n",
    "\n",
    "We'll need a few specific functions to process the predicted values with the structure of the Cell Ontology. We'll define these here. Full details of each function are found in each space. \n",
    "\n",
    "- output_probability_tensor: convolves the predicted classification outputs with the ontology hierarchy to get predicted normalized probabilities for all parent nodes\n",
    "- target_probability_tensor: convolves the known target values with the ontology hierarchy to get target probabilities for all parent nodes\n",
    "- build_mask_tensor_for_batch : builds a masking tensor from cell_parent_mask specific to the targets for each batch of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(x_data):\n",
    "    '''\n",
    "    This function takes the input x_data, transforms the data with log(1+x) and \n",
    "    returns the transformed data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_data : scipy matrix\n",
    "        scipy sparse CSR matrix  \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_data : SciPy Matrix\n",
    "        scipy sparse CSR matrix\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # np.log takes the natural log\n",
    "    x_data.data = np.log(1+ x_data.data)\n",
    "\n",
    "    return x_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probability_tensor(outputs,ontology_df):\n",
    "    '''\n",
    "    Function to convolve the predicted classification outputs with the ontology heirarchy to\n",
    "    get predicted normalized probabilities for all parents. \n",
    "    Precursur to loss calculation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    outputs : tensor\n",
    "        PyTorch tensor of shape [a,b] where a = number of cells and b = number of target leafs\n",
    "        This tensor is the result of the classification in the neural network\n",
    "                \n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where rows are parent labels and columns are leafs\n",
    "        values indicate if parent node is an ancestor of leaf node\n",
    "\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    sum_probability_tensor : tensor\n",
    "        PyTorch tensor of shape [c,a] where c = number of cell ontology IDs and a = number of cells\n",
    "        Each entry is the summed predicted probability for that cell ID\n",
    "\n",
    "    '''\n",
    "        \n",
    "    # convert the dataframe to a pytorch tensor\n",
    "    ontology_tensor = torch.FloatTensor(ontology_df.values)\n",
    "    ontology_tensor = ontology_tensor.to(device)\n",
    "        \n",
    "    # convolve the ontology tensor with the predicted outputs\n",
    "    # ontology tensor is shape ij, where i = parent IDs, j = probability for leaf IDs\n",
    "    # output tensor is shape kj, where k = number of cells classified, j = probability for leaf IDs\n",
    "    # probability tensor is shape ijk\n",
    "    \n",
    "    # if there is only a single column for the ontology, change shape to match expected value\n",
    "    if len(ontology_tensor.shape) == 1:\n",
    "        ontology_tensor = ontology_tensor.unsqueeze(1)    \n",
    "    #print(ontology_tensor.shape)\n",
    "    #print(outputs.shape)\n",
    "    probability_tensor = torch.einsum('ij,kj->ijk',ontology_tensor,outputs)\n",
    "    #print('prob_tensor', probability_tensor.shape)\n",
    "    \n",
    "    # sum across leafs to get the predicted probability, by cell, for each\n",
    "    # parent \n",
    "    # sum_probability_tensor is shape ik, where i = parent IDs, k = number of cells\n",
    "    sum_probability_tensor = torch.sum(probability_tensor,dim=1,dtype=float)\n",
    "    \n",
    "    ##sum_masked_probability_tensor = sum_probability_tensor * batch_masking_tensor\n",
    "    #print('sum masked',sum_masked_probability_tensor.shape)\n",
    "    #print('sum masked',sum_masked_probability_tensor.sum(dim=1))\n",
    "    # ensure that the max value is 1 because of floating point issues\n",
    "    # if we don't do this, we can run into errors with the binary cross entropy\n",
    "    ##sum_masked_probability_tensor = torch.where(sum_masked_probability_tensor > 1, 1.0, sum_masked_probability_tensor )\n",
    "    sum_probability_tensor = torch.where(sum_probability_tensor > 1, 1.0, sum_probability_tensor )\n",
    "\n",
    "    return sum_probability_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e3aa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mask_tensor_for_batch(cell_parent_mask,y_batch,min_encoded_value,max_encoded_value):\n",
    "    '''\n",
    "    For each batch, this function builds the correct masking tensor based on which\n",
    "    values of the cell ontology we want to include given the target. It returns aa \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cell_parent_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "                \n",
    "    y_batch : tensor\n",
    "        tensor with encoded target values for current batch of data\n",
    "        \n",
    "    min_encoded_value : int\n",
    "        the minimum encoded value from the full set of target values. Typically -9999\n",
    "        \n",
    "    max_encoded_value : int\n",
    "        the maximum encoded value from the full set of target values. Depends on number\n",
    "        of leaf targets in the dataset\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    batch_masking_tensor : tensor\n",
    "        PyTorch tensor of shape [c,a] where c = number of cell ontology IDs and a = number of cells\n",
    "        Binary tensor to zero out probabilities we do not want\n",
    "    \n",
    "    '''\n",
    "    # 1) from y_batch, we need to get the indices we'll use to select from cell_parent_mask\n",
    "    #.   all the positive values are each, but we need to convert the negative values to\n",
    "    #.   correspond with the (positive) index they would otherwise be. Then save to a tensor\n",
    "    \n",
    "    for value in y_batch:\n",
    "        if value >= 0:\n",
    "            new_value = value\n",
    "        else:\n",
    "            new_value = (value - min_encoded_value) + max_encoded_value + 1\n",
    "        try:\n",
    "            converted_y_batch = torch.cat((converted_y_batch,new_value.reshape(1)),dim=0)\n",
    "        except:\n",
    "            converted_y_batch = new_value.reshape(1)\n",
    "    \n",
    "    \n",
    "    # 2) use the y_batch converted values to build a new tensor from cell_parent_mask\n",
    "    #.    that is the mask we will use for this batch of values.\n",
    "    #.    return this tensor\n",
    "\n",
    "    cell_parent_mask = cell_parent_mask.to(device)\n",
    "    batch_masking_tensor = torch.index_select(cell_parent_mask,1,converted_y_batch)\n",
    "    #print(batch_masking_tensor.sum(dim=0))\n",
    "    \n",
    "    return(batch_masking_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_probability_tensor(target_values,ontology_df,mapping_dict):\n",
    "    '''\n",
    "    Function to convolve the known target values with the ontology heirarchy\n",
    "    Precursur to loss calculation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_values : tensor\n",
    "        PyTorch tensor of shape [a] where a = number of cells\n",
    "                \n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where rows are parent labels and columns are leafs\n",
    "        values indicate if parent node is an ancestor of leaf node\n",
    "\n",
    "    mapping_dict : dictionary\n",
    "        dictionary mapping the Cell Ontology IDs (keys) to the encoded values (values)\n",
    "        Values >= 0 are leaf nodes\n",
    "        Values < 0 are internal nodes\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    target_tensor : tensor\n",
    "        PyTorch tensor of shape [c,a] where c = number of cell ontology IDs and a = number of cells\n",
    "        Each entry is the summed predicted probability for that cell ID\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # loop through target values, pick out corresponding column of ontology_df\n",
    "    # append that to a tensor\n",
    "    \n",
    "    # invert the mapping dict so that we can select columns by CELL TYPE ID\n",
    "    inv_mapping_dict = {v: k for k,v in mapping_dict.items()}\n",
    "\n",
    "    for count, target_value in enumerate(target_values):\n",
    "        # get the cell ID from the inverted mapping dictionary based on the encoded value\n",
    "        target_cell_id = inv_mapping_dict[target_value.item()]\n",
    "        \n",
    "        # look up the correct column by the Cell ID. get those column values and convert\n",
    "        # to a tensor\n",
    "        sub_target_tensor = torch.tensor(ontology_df.loc[:,target_cell_id].values,dtype=float).reshape(-1,1)\n",
    "        \n",
    "        if count == 0 :\n",
    "            target_tensor = sub_target_tensor\n",
    "        else:\n",
    "            # set requires_grad so that we can track\n",
    "            target_tensor = torch.cat((target_tensor,sub_target_tensor),1).requires_grad_()\n",
    "    #print('target tensor shape',target_tensor.shape)\n",
    "    #print('batch_masking_tensor',batch_masking_tensor.shape)\n",
    "    ###masked_target_tensor = target_tensor * batch_masking_tensor\n",
    "    ##print('masked target tensor',masked_target_tensor.shape)\n",
    "    \n",
    "    target_tensor = target_tensor.to(device)\n",
    "    ###masked_target_tensor = masked_target_tensor.to(device)\n",
    "    \n",
    "    \n",
    "    return target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf08074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85f3a8f",
   "metadata": {},
   "source": [
    "# Marginalization Classification \n",
    "\n",
    "- Based on Dahll, et al., Hierarchical Image Classification using Entailment Cone Embeddings, CVPR 202\n",
    "- https://arxiv.org/pdf/2004.03459.pdf\n",
    "- Thesis slides: https://ankitdhall.github.io/publication/learning-representations-for-images-with-hierarchical-labels/master_thesis.pdf\n",
    "- First Author website: https://ankitdhall.github.io/project/learning-representations-for-images-with-hierarchical-labels/\n",
    "\n",
    "Important Details:\n",
    "- we use mini-batch learning, with the batch size set by the user\n",
    "- we model each batch of data at once, then split into leaf and internal nodes, based on the values in y_batch\n",
    "- we calculate the loss two different ways, then sum to get the total loss\n",
    "- we calculate and save the loss, accuracy, and F1 score for metrics to review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1df033",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_leafs = nn.CrossEntropyLoss(reduction='mean')\n",
    "testoutput = torch.empty((0,3),dtype=torch.float)\n",
    "testy = torch.empty((0,3),dtype=torch.float)\n",
    "loss_train_leafs = criterion_leafs(testoutput, testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8771ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "testoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e57bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zerotensor = torch.tensor(0,dtype=torch.int64)\n",
    "zerotensor = zerotensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f596f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zerotensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f553c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss for a given batch for graph\n",
    "batch_losses = []\n",
    "# batch numbers to use as x-axis in graph\n",
    "x_vals = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marginalization_classification_manual_batch(train_dataloader,val_dataloader,num_epochs,ontology_leaf_df, \n",
    "                                                batch_size,internal_values,mapping_dict,\n",
    "                                               ontology_df, threshold, cell_parent_mask,encoding_mapper):\n",
    "    '''\n",
    "    Performs training and validation simultaneously to allow visualization of model performance \n",
    "    per epoch. Accounts for entire tree structure of the ontology by classifying to the leaf nodes, \n",
    "    propogating the probabilities across the ontology, and calculating the loss for both the leafs \n",
    "    and parent nodes. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_dataloader : DataLoader object\n",
    "        DataLoader object for training data from census_ml.experiment_dataloader for \n",
    "        streaming data from disk from the CZI cell_census API\n",
    "        similar to PyTorach DataLoader but specific to working with Cell Census data\n",
    "    \n",
    "    val_dataloader : DataLoader object\n",
    "        DataLoader object for validation data from census_ml.experiment_dataloader for \n",
    "        streaming data from disk from the CZI cell_census API\n",
    "        similar to PyTorach DataLoader but specific to working with Cell Census \n",
    "        \n",
    "    num_epochs : int\n",
    "        integer specify the number of epochs\n",
    "        \n",
    "    ontology_leaf_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are onlys leafs in portion of ontology being queried. \n",
    "        Differs from ontology_df in that columns do not include any internal nodes.\n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "    \n",
    "    batch_size : int\n",
    "        integer specificying the number of samples processed before the model\n",
    "        is updated\n",
    "        \n",
    "    internal_values : list\n",
    "        list of Cell Ontology IDs for internal nodes included in the dataset\n",
    "        \n",
    "    mapping_dict : dict\n",
    "        dictionary mapping the Cell Ontology IDs (keys) to the encoded values (values)\n",
    "        Values >= 0 are leaf nodes\n",
    "        Values < 0 are internal nodes\n",
    "    \n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are leafs and internal nodes in portion of ontology being \n",
    "        queried. \n",
    "        Differs from ontology_leaf_df in that columns include both leaf and internal node values\n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "        \n",
    "    threshold : float\n",
    "        value between 0 and 1 to set for making predictions. If the predicted probability is\n",
    "        equal to or greater than threshold, we consider that a true prediction\n",
    "        \n",
    "    cell_parent_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "\n",
    "    encoding_mapper : dictionary\n",
    "        dictionary mapping from encoder_mapping_dict (keys) to mapping_dict (values)\n",
    "        used to differentiate leaf values and internal values \n",
    "        \n",
    "    Returns\n",
    "    -------    \n",
    "    marginalization_dict : dictionary\n",
    "        dictionary containing results from each epoch of the neural network\n",
    "    \n",
    "    Keys and Values include:\n",
    "    \n",
    "        accuracy_train_leaf_hist : list\n",
    "            list containing accuracy for leaf values for the training set per epoch\n",
    "            \n",
    "        accuracy_train_internal_hist : list\n",
    "            list containing accuracy for internal values for the trainig set per epoch\n",
    "        \n",
    "        loss_train_hist: list\n",
    "            list containing total loss values for the training set per epoch\n",
    "            \n",
    "        loss_train_leaf_hist : list\n",
    "            list containing loss values for leaf nodes for the training set per epoch\n",
    "            \n",
    "        loss_train_parents_hist : list\n",
    "            list containing loss values for parent nodes for the training set per epoch\n",
    "        \n",
    "        loss_train_internal_hist : list\n",
    "            list containing loss values for internal nodes for the training set per epoch\n",
    "        \n",
    "        accuracy_val_leaf_hist : list \n",
    "            list containing accuracy for leaf values for the validation set per epoch\n",
    "        \n",
    "        accuracy_val_internal_hist : list\n",
    "            list containing accuracy for internal values for the validation set per epoch\n",
    "        \n",
    "        loss_val_hist : list\n",
    "            list containing total loss values for the validation set per epoch\n",
    "            \n",
    "        loss_val_leaf_hist : list\n",
    "            list containing loss values for leaf nodes for the validation set per epoch\n",
    "            \n",
    "        loss_val_parents_hist : list\n",
    "            list containing loss values for parent nodes for the validation set per epoch\n",
    "            \n",
    "        loss_val_internal_hist : list\n",
    "            list containing loss values for internal nodes for the validation set per epoch\n",
    "        \n",
    "        f1_score_train_leaf : list\n",
    "            list containing Macro F1 score for leaf nodes for training set per epoch \n",
    "        \n",
    "        f1_score_val_leaf : listba\n",
    "            list containing Macro F1 score for leaf nodes for validation set per epoch \n",
    "        \n",
    "        best_output : tensor\n",
    "            PyTorch tensor containing the predicted probabilites for the most accurate\n",
    "            epoch\n",
    "            \n",
    "        best_state_dict : dictionary\n",
    "            Pytorch state_dict that contains the parameters for the best fitting models\n",
    "    '''\n",
    "    \n",
    "    # initialize variables for saving values\n",
    "    accuracy_train_leaf_hist = []\n",
    "    accuracy_train_internal_hist = []\n",
    "    loss_train_leaf_hist = []\n",
    "    loss_train_parents_hist = []\n",
    "    loss_train_internal_hist = []\n",
    "    loss_train_hist = []\n",
    "    \n",
    "    accuracy_val_leaf_hist = []\n",
    "    accuracy_val_internal_hist = []\n",
    "    loss_val_hist = []\n",
    "    loss_val_leaf_hist = []\n",
    "    loss_val_parents_hist = []\n",
    "    loss_val_internal_hist = []\n",
    "    \n",
    "    f1_score_train_leaf = []\n",
    "    f1_score_val_leaf = []\n",
    "    \n",
    "    f1_score_train_parent = []\n",
    "    f1_score_val_parent = []\n",
    "    \n",
    "    best_accuracy = - np.inf\n",
    "    best_weights = None\n",
    "    \n",
    "    # get the list of leaf labels\n",
    "    leaf_label_list = [value for (key,value) in mapping_dict.items() if value >= 0]\n",
    "\n",
    "    # get the min and max encoded values\n",
    "    min_encoded_value = min(mapping_dict.values()) #min(y_train).item()\n",
    "    max_encoded_value = max(mapping_dict.values()) #max(y_train).item()\n",
    "\n",
    "    # initialize network\n",
    "    clf = Network()\n",
    "    clf.to(device)\n",
    "#     clf.apply(init_weights)\n",
    "\n",
    "    # define loss and optimizer\n",
    "    # we use two different loss methods for the leafs and parents\n",
    "    # use Cross Entory Loss for leafs, because those probabilities are normalized\n",
    "    #     and it is thus a multi-class problem\n",
    "    # Use BCELoss for the parents because this is a multi-label problem\n",
    "    #     and the probabilities are normalized, so we don't need BCELossWithLogits\n",
    "    # initialize the leaf loss here\n",
    "    # because of how we weight the parent loss, we will have to initialize that\n",
    "    # loss on each iteration because the weighting will change.\n",
    "    criterion_leafs = nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    adam_lr = 5e-4\n",
    "#     optimizer = torch.optim.Adam(clf.parameters(), lr=adam_lr)#, amsgrad=True, eps=1e-5)\n",
    "    optimizer = torch.optim.SGD(clf.parameters(), lr=adam_lr)\n",
    "#     scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=5)\n",
    "#     scheduler = lr_scheduler.StepLR(optimizer, step_size=5,gamma=0.5)\n",
    "\n",
    "    batches_for_graph = 0\n",
    "\n",
    "    #start_epoch = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print()\n",
    "        print(\"************** EPOCH\", epoch, \"**************\")\n",
    "        \n",
    "        # TRAINING\n",
    "        #print('Begin Training...')\n",
    "        clf.train()\n",
    "        \n",
    "        # sums of the losses to take averages of later\n",
    "        running_train_loss = 0.0\n",
    "        running_train_leaf_loss = 0.0\n",
    "        running_train_parent_loss = 0.0\n",
    "        \n",
    "        # keep track of batches for average later \n",
    "        batches = 0\n",
    "                \n",
    "        # set up manual batches - from https://stackoverflow.com/questions/45113245/how-to-get-mini-batches-in-pytorch-in-a-clean-and-efficient-way\n",
    "        #permutation = torch.randperm(X_train.size()[0]).to(device)\n",
    "        #permutation_cpu = permutation.cpu() # we want the same permutations, but need one copy on the cpu\n",
    "        \n",
    "        print('start train batching')\n",
    "        start_batch = time.time()\n",
    "        #for i in range(0,X_train.size()[0], batch_size):\n",
    "        for i, train_batch in enumerate(train_dataloader):\n",
    "            \n",
    "            batches += 1\n",
    "            batches_for_graph += 1\n",
    "        \n",
    "            if (i) % 100 == 0:\n",
    "                print('on batch', i, 'running time', (time.time()-start_batch))\n",
    "\n",
    "            #print(i)\n",
    "            #indices = permutation[i:i+batch_size]\n",
    "            #indices_cpu = permutation_cpu[i:i+batch_size]\n",
    "            #X_batch, y_batch = X_train[indices], y_train[indices] # doesn't work for sparse tensors\n",
    "\n",
    "            #X_batch = torch.index_select(X_train,0,indices_cpu).to(device)\n",
    "\n",
    "            #y_batch = torch.index_select(y_train,0,indices)#.to(device)\n",
    "\n",
    "            X_batch, y_batch = train_batch\n",
    "            \n",
    "            # change dtype to float\n",
    "            X_batch = X_batch.float()\n",
    "            \n",
    "            # transform the data with log(1+x)\n",
    "            X_batch = transform_data(X_batch)\n",
    "            \n",
    "            # move to device\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            # select the encoded values from the experiment_datapipe\n",
    "            y_batch = y_batch[:,0]\n",
    "\n",
    "            # then map the values from the datapipe encoded values to the\n",
    "            # encoded values from the Ontology/mapping_dict\n",
    "            \n",
    "            #encoding_mapper\n",
    "            y_batch = torch.tensor([encoding_mapper[x.item()] for x in y_batch])\n",
    "            \n",
    "            # move to device\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # check that tensors are on gpu. if on gpu, get_device returns 0, if on cpu, returns -1\n",
    "            #print(X_batch.get_device())\n",
    "            #print(y_batch.get_device())\n",
    "            \n",
    "            # set optimizer to zero grad to remove previous epoch gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # make predictions for this batch\n",
    "            #if epoch == 0:\n",
    "            #    outputs_train = clf_nosoftmax(X_batch.float()) # might need to change to X_train.float()\n",
    "            #else:\n",
    "            outputs_train = clf(X_batch.float()) # might need to change to X_train.float()\n",
    "            \n",
    "            ######\n",
    "            # create mask to separate leaf and internal nodes\n",
    "            ######\n",
    "            output_train_leaf = outputs_train[y_batch >= 0]\n",
    "            y_batch_leaf = y_batch[y_batch >= 0]\n",
    "    \n",
    "            #output_train_internal = outputs_train[y_batch < 0]\n",
    "            #y_batch_internal = y_batch[y_batch < 0]\n",
    "\n",
    "            # calculate loss for just the leafs\n",
    "            loss_train_leafs = criterion_leafs(output_train_leaf, y_batch_leaf)\n",
    "            loss_train_leafs = loss_train_leafs * leaf_weight\n",
    "    \n",
    "            # https://stackoverflow.com/questions/67730325/using-weights-in-crossentropyloss-and-bceloss-pytorch\n",
    "#             loss_train_leafs_0 = nn.CrossEntropyLoss(output_train_leaf, y_batch_leaf, reduce=False)\n",
    "#             loss_train_leafs = torch.mean(loss_train_leafs_0*leaf_weights)\n",
    "            \n",
    "            # if we have no leaf values in the batch, the leaf loss evaluates to NaN.\n",
    "            # in this case, set it to 0\n",
    "            if torch.isnan(loss_train_leafs).item():\n",
    "                loss_train_leafs = torch.tensor(0,dtype=torch.int64)\n",
    "                loss_train_leafs = loss_train_leafs.to(device)\n",
    "\n",
    "            # get the masking tensor for this batch of cells\n",
    "            # for calculating the internal loss\n",
    "            # we initialize BCE loss every batch because the mask changes based on which cells are\n",
    "            # included and ordered for this batch\n",
    "            batch_train_masking_tensor = build_mask_tensor_for_batch(cell_parent_mask,y_batch,min_encoded_value,max_encoded_value)\n",
    "            criterion_parents = nn.BCELoss(weight=batch_train_masking_tensor,reduction='mean')\n",
    "\n",
    "            \n",
    "            # calculate the loss for the parents of the cells that are leafs\n",
    "            output_train_parent_prob = output_probability_tensor(outputs_train,ontology_leaf_df)\n",
    "            target_train_parent_prob = target_probability_tensor(y_batch,ontology_df,mapping_dict)\n",
    "\n",
    "            loss_train_parents = criterion_parents(output_train_parent_prob,target_train_parent_prob)\n",
    "\n",
    "            #######\n",
    "            # calculate the loss for the cells that are internal nodes\n",
    "            ##### total_accuracy_cell, total_number_of_cells\n",
    "            \n",
    "            #loss_train_internal, batch_accuracy_internal, batch_numbers_internal = internal_node_loss(\n",
    "            #                                        output_train_internal,y_batch_internal,\n",
    "            #                                         internal_values,mapping_dict,ontology_df,\n",
    "            #                                        ontology_leaf_df,criterion_parents,threshold)\n",
    "            \n",
    "            if i > 0:\n",
    "                last_loss = loss_train\n",
    "            \n",
    "            # sum the loss for both leafs and parents\n",
    "            loss_train = loss_train_leafs + loss_train_parents #+ loss_train_internal\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(\"Batch\", i, \"loss:\", loss_train.item())\n",
    "                print(\"leaf:\", loss_train_leafs.item(), \"and parents:\", loss_train_parents.item())\n",
    "            \n",
    "            # every 100 batches of the epoch log the loss and\n",
    "            # batch number for graphing later\n",
    "            if (batches) % 100 == 0:\n",
    "                batch_losses.append(loss_train.item())\n",
    "                x_vals.append(batches_for_graph)\n",
    "\n",
    "            # backward propagation\n",
    "            loss_train.backward()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "             \n",
    "            running_train_loss += loss_train\n",
    "            running_train_leaf_loss += loss_train_leafs\n",
    "            running_train_parent_loss += loss_train_parents\n",
    "           \n",
    "            # save predictions\n",
    "            _, train_leaf_pred_per_epoch = output_train_leaf.max(dim=1)\n",
    "\n",
    "            # calculate accuracy for internal cells\n",
    "            #####\n",
    "            # need to update this with the weighting somehow!!!!\n",
    "            #####\n",
    "            train_batch_accuracy = multilabel_accuracy(output_train_parent_prob,target_train_parent_prob,\n",
    "                                                      threshold=threshold,criteria='hamming')\n",
    "            \n",
    "            # save the number of cells for batch, for use in weighting when\n",
    "            # determining the overall accuracy per epoch\n",
    "            train_batch_number_of_cells = output_train_parent_prob.shape[1]\n",
    "                        \n",
    "            # check size of internal tensors. if only 1 internal cell\n",
    "            # reshape and detach\n",
    "            # else just detach\n",
    "            \n",
    "            ##if len(batch_accuracy_internal.size()) == 0:\n",
    "            ##    batch_accuracy_internal_shaped = batch_accuracy_internal.detach().reshape(1)\n",
    "            ##    batch_numbers_internal_shaped = batch_numbers_internal.detach().reshape(1)\n",
    "            ##else:\n",
    "            ##    batch_accuracy_internal_shaped = batch_accuracy_internal.detach()\n",
    "            ##    batch_numbers_internal_shaped = batch_numbers_internal.detach()\n",
    "                \n",
    "            \n",
    "            if i == 0:\n",
    "                train_leaf_pred_total = train_leaf_pred_per_epoch.detach()\n",
    "                y_train_leaf_total = y_batch_leaf.detach()\n",
    "                train_batch_accuracy_internal = train_batch_accuracy.reshape(1)\n",
    "                train_total_number_of_cells = torch.tensor(train_batch_number_of_cells).reshape(1)\n",
    "                output_train_probabilities = output_train_leaf.detach()\n",
    "                train_parent_pred_total = output_train_parent_prob.detach()\n",
    "                train_parent_true_total = target_train_parent_prob.detach()\n",
    "            else:\n",
    "                train_leaf_pred_total = torch.cat((train_leaf_pred_total,train_leaf_pred_per_epoch.detach()),0)\n",
    "                y_train_leaf_total = torch.cat((y_train_leaf_total,y_batch_leaf.detach()),0)\n",
    "                train_batch_accuracy_internal = torch.cat((train_batch_accuracy_internal,train_batch_accuracy.reshape(1)),0)\n",
    "                train_total_number_of_cells = torch.cat((train_total_number_of_cells,torch.tensor(train_batch_number_of_cells).reshape(1)),0)\n",
    "                output_train_probabilities = torch.cat((output_train_probabilities,output_train_leaf.detach()),0)\n",
    "                train_parent_pred_total = torch.cat((train_parent_pred_total,output_train_parent_prob.detach()),1)\n",
    "                train_parent_true_total = torch.cat((train_parent_true_total,target_train_parent_prob.detach()),1)\n",
    "                #print('train parent pred',train_parent_pred_total.shape)\n",
    "                #print('train parent true',train_parent_true_total.shape)\n",
    "                \n",
    "            # exit after 1 loop so we can test validation code\n",
    "            #break\n",
    "                            \n",
    "            # calculate total epoch accuracy for internal nodes\n",
    "            #epoch_internal_accuracy = / train_numbers_internal.sum() * 100\n",
    "            \n",
    "            #correct_train += (train_pred_per_epoch == y_batch).sum().item()\n",
    "            #y_length += len(y_batch)\n",
    "            \n",
    "            # calculate F1 score\n",
    "            #f1_val_score_epoch = f1_score(train_pred_per_epoch.cpu(),y_batch.cpu(),labels=output_dim,average='weighted',zero_division=np.nan)\n",
    "\n",
    "#         scheduler.step()\n",
    "        \n",
    "        print('done with training')\n",
    "        \n",
    "        # calculate the average losses over each batch to log as the epoch losses\n",
    "        avg_train_loss = running_train_loss / float(batches)\n",
    "        avg_train_leaf_loss = running_train_leaf_loss / float(batches)\n",
    "        avg_train_parent_loss = running_train_parent_loss / float(batches)\n",
    "        \n",
    "        train_total_number_of_cells = train_total_number_of_cells.to(device)\n",
    "        \n",
    "        correct_train_leaf = (train_leaf_pred_total == y_train_leaf_total).sum().item()\n",
    "        accuracy_train_leaf_hist.append(correct_train_leaf / train_leaf_pred_total.shape[0] * 100.)\n",
    "    \n",
    "        correct_train_internal = (train_batch_accuracy_internal * train_total_number_of_cells).sum()\n",
    "        accuracy_train_internal = (correct_train_internal / train_total_number_of_cells.sum() * 100.).item()\n",
    "        accuracy_train_internal_hist.append(accuracy_train_internal)\n",
    "        \n",
    "        loss_train_hist.append(avg_train_loss)\n",
    "        loss_train_leaf_hist.append(avg_train_leaf_loss)\n",
    "        loss_train_parents_hist.append(avg_train_parent_loss)\n",
    "        \n",
    "        # save f1 score\n",
    "        # use average = weighted to account for label imbalance\n",
    "        # use zero_division = np.nan to exclude labels where all \n",
    "        #       predictions and labels are negative\n",
    "        \n",
    "        # (Jake note) For some reason I couldn't get zero_division=np.nan to work.\n",
    "        # Probably just a versioning thing.\n",
    "        # Switching zero_division to \"warn\" changes the final value but still works ok\n",
    "#         f1_train_leaf_score = f1_score(y_train_leaf_total.cpu(), train_leaf_pred_total.cpu(),\n",
    "#                                   labels=leaf_label_list,average='weighted',zero_division=np.nan)\n",
    "        f1_train_leaf_score = f1_score(y_train_leaf_total.cpu(), train_leaf_pred_total.cpu(),\n",
    "                                  labels=leaf_label_list,average='weighted',zero_division=\"warn\")\n",
    "        f1_score_train_leaf.append(f1_train_leaf_score)\n",
    "        \n",
    "        # for the F1 score for the internal nodes, we need to first turn the probabilities\n",
    "        # into predictions using our threshold value\n",
    "        train_parent_pred_total_thresholded = torch.where(train_parent_pred_total > threshold,1.0,0.0)\n",
    "        \n",
    "#         f1_train_parent_score = f1_score(train_parent_true_total.cpu(),train_parent_pred_total_thresholded.cpu(),\n",
    "#                                         average='weighted',zero_division=np.nan)\n",
    "        f1_train_parent_score = f1_score(train_parent_true_total.cpu(),train_parent_pred_total_thresholded.cpu(),\n",
    "                                        average='weighted',zero_division=\"warn\")\n",
    "        f1_score_train_parent.append(f1_train_parent_score)\n",
    "        \n",
    "        # set up validation\n",
    "        correct_val = 0\n",
    "        y_val_length = 0\n",
    "        \n",
    "        # keep track of batches for average later \n",
    "        batches = 0\n",
    "        \n",
    "        # sums of the losses to take averages of later\n",
    "        running_val_loss = 0.0\n",
    "        running_val_leaf_loss = 0.0\n",
    "        running_val_parent_loss = 0.0\n",
    "        \n",
    "        print('start validation')\n",
    "        with torch.no_grad():\n",
    "            clf.eval()\n",
    "            \n",
    "            # set up manual batches\n",
    "            # we don't need to randomly permute the validation set, but\n",
    "            # this will provide consistency with the above.\n",
    "            # for simplicity, let's use the same batch size\n",
    "            #permutation_val = torch.randperm(X_val.size()[0]).to(device)\n",
    "            #start_val = time.time()\n",
    "            \n",
    "            #for i in range(0,X_val.size()[0],batch_size):\n",
    "            print('start validation batching')\n",
    "            start_batch = time.time()\n",
    "            for i, val_batch in enumerate(val_dataloader):\n",
    "                \n",
    "                batches += 1\n",
    "                \n",
    "                if (i) % 100 == 0:\n",
    "                    print('on batch', i, 'running time', (time.time()-start_batch))\n",
    "\n",
    "                #indices_val = permutation_val[i:i+batch_size]\n",
    "                \n",
    "                #X_val_batch = torch.index_select(X_val,0,indices_val)\n",
    "                #y_val_batch = torch.index_select(y_val,0,indices_val)\n",
    "                \n",
    "                X_val_batch, y_val_batch = val_batch\n",
    "            \n",
    "                # change dtype to float\n",
    "                X_val_batch = X_val_batch.float()\n",
    "\n",
    "                # transform the data with log(1+x)\n",
    "                X_val_batch = transform_data(X_val_batch)\n",
    "                \n",
    "                # move to device\n",
    "                X_val_batch = X_val_batch.to(device)\n",
    "                \n",
    "                # select the encoded values from the experiment_datapipe\n",
    "                y_val_batch = y_val_batch[:,0]\n",
    "\n",
    "                # then map the values from the datapipe encoded values to the\n",
    "                # encoded values from the Ontology/mapping_dict\n",
    "\n",
    "                #encoding_mapper\n",
    "                y_val_batch = torch.tensor([encoding_mapper[x.item()] for x in y_val_batch])\n",
    "                \n",
    "                # move to device\n",
    "                y_val_batch = y_val_batch.to(device)\n",
    "                \n",
    "                # calculate output by running through the network\n",
    "                outputs_val = clf(X_val_batch)\n",
    "            \n",
    "                ######\n",
    "                # create mask to separate leaf and internal nodes\n",
    "                ######\n",
    "                output_val_leaf = outputs_val[y_val_batch >= 0]\n",
    "                y_val_batch_leaf = y_val_batch[y_val_batch >= 0]\n",
    "            \n",
    "                # calculate loss for just the leafs\n",
    "                loss_val_leafs = criterion_leafs(output_val_leaf, y_val_batch_leaf)\n",
    "        \n",
    "                # if we have no leaf values in the batch, the leaf loss evaluates to NaN.\n",
    "                # in this case, set it to 0\n",
    "                if torch.isnan(loss_val_leafs).item():\n",
    "                    loss_val_leafs = torch.tensor(0,dtype=torch.int64)\n",
    "                    loss_val_leafs = loss_val_leafs.to(device)\n",
    "\n",
    "                # get the masking tensor for this batch of cells\n",
    "                # for calculating the internal loss\n",
    "                # we initialize BCE loss every batch because the mask changes based on which cells are\n",
    "                # included and ordered for this batch\n",
    "                batch_val_masking_tensor = build_mask_tensor_for_batch(cell_parent_mask,y_val_batch,min_encoded_value,max_encoded_value)\n",
    "                criterion_parents = nn.BCELoss(weight=batch_val_masking_tensor,reduction='mean')\n",
    "                \n",
    "                # calculate the loss for the parents of the leafs\n",
    "                output_val_parent_prob = output_probability_tensor(outputs_val,ontology_leaf_df)\n",
    "                target_val_parent_prob = target_probability_tensor(y_val_batch,ontology_df,mapping_dict)\n",
    "                \n",
    "                loss_val_parents = criterion_parents(output_val_parent_prob,target_val_parent_prob)\n",
    "\n",
    "                # sum the loss for both leafs and parents\n",
    "                loss_val = loss_val_leafs + loss_val_parents\n",
    "            \n",
    "                running_val_loss += loss_val\n",
    "                running_val_leaf_loss += loss_val_leafs\n",
    "                running_val_parent_loss += loss_val_parents\n",
    "            \n",
    "                # get the predictions\n",
    "                __, predicted_leaf_val_per_epoch = output_val_leaf.max(dim=1)            \n",
    "\n",
    "                # calculate accuracy for internal cells\n",
    "                #####\n",
    "                # need to update this with the weighting somehow!!!!\n",
    "                #####\n",
    "                val_batch_accuracy = multilabel_accuracy(output_val_parent_prob,target_val_parent_prob,\n",
    "                                                          threshold=threshold,criteria='hamming')\n",
    "\n",
    "                # save the number of cells for batch, for use in weighting when\n",
    "                # determining the overall accuracy per epoch\n",
    "                val_batch_number_of_cells = output_val_parent_prob.shape[1]\n",
    "                                \n",
    "                if i == 0:\n",
    "                    val_leaf_pred_total = predicted_leaf_val_per_epoch.detach()\n",
    "                    y_leaf_val_total = y_val_batch_leaf.detach()\n",
    "                    val_batch_accuracy_internal = val_batch_accuracy.reshape(1)\n",
    "                    val_total_number_of_cells = torch.tensor(val_batch_number_of_cells).reshape(1)\n",
    "                    val_parent_pred_total = output_val_parent_prob.detach()\n",
    "                    val_parent_true_total = target_val_parent_prob.detach()\n",
    "\n",
    "                else:\n",
    "                    val_leaf_pred_total = torch.cat((val_leaf_pred_total,predicted_leaf_val_per_epoch.detach()),0)\n",
    "                    y_leaf_val_total = torch.cat((y_leaf_val_total,y_val_batch_leaf.detach()),0)\n",
    "                    val_batch_accuracy_internal = torch.cat((val_batch_accuracy_internal,val_batch_accuracy.reshape(1)),0)\n",
    "                    val_total_number_of_cells = torch.cat((val_total_number_of_cells,torch.tensor(val_batch_number_of_cells).reshape(1)),0)\n",
    "                    val_parent_pred_total = torch.cat((val_parent_pred_total,output_val_parent_prob.detach()),1)\n",
    "                    val_parent_true_total = torch.cat((val_parent_true_total,target_val_parent_prob.detach()),1)\n",
    "\n",
    "                # exit after 1 loop so we can test  code\n",
    "#                 break\n",
    "\n",
    "            print('done with validation')\n",
    "            val_total_number_of_cells = val_total_number_of_cells.to(device)\n",
    "            \n",
    "            correct_val_leaf = (val_leaf_pred_total == y_leaf_val_total).sum().item()\n",
    "            #print(val_leaf_pred_total.shape)\n",
    "            accuracy_val_leaf_hist.append(correct_val_leaf / val_leaf_pred_total.shape[0] * 100.)\n",
    "\n",
    "            correct_val_internal = (val_batch_accuracy_internal * val_total_number_of_cells).sum()\n",
    "            accuracy_val_internal = (correct_val_internal / val_total_number_of_cells.sum() * 100.).item()\n",
    "            accuracy_val_internal_hist.append(accuracy_val_internal)\n",
    "\n",
    "            # calculate the average losses over each batch to log as the epoch losses\n",
    "            avg_val_loss = running_val_loss / batches\n",
    "            avg_val_leaf_loss = running_val_leaf_loss / batches\n",
    "            avg_val_parent_loss = running_val_parent_loss / batches\n",
    "\n",
    "            loss_val_hist.append(avg_val_loss)\n",
    "            loss_val_leaf_hist.append(avg_val_leaf_loss)\n",
    "            loss_val_parents_hist.append(avg_val_parent_loss)\n",
    "            \n",
    "            # save f1 score\n",
    "            # use average = weighted to account for label imbalance\n",
    "            # use zero_division = np.nan to exclude labels where all \n",
    "            #       predictions and labels are negative\n",
    "            \n",
    "            # (Jake note) For some reason I couldn't get zero_division=np.nan to work.\n",
    "            # Probably just a versioning thing.\n",
    "            # Switching zero_division to \"warn\" changes the final value but still works ok.\n",
    "#             f1_val_leaf_score = f1_score(y_leaf_val_total.cpu(),val_leaf_pred_total.cpu(),\n",
    "#                                     labels=leaf_label_list,average='weighted',zero_division=np.nan)\n",
    "            f1_val_leaf_score = f1_score(y_leaf_val_total.cpu(),val_leaf_pred_total.cpu(),\n",
    "                                    labels=leaf_label_list,average='weighted',zero_division=\"warn\")\n",
    "            f1_score_val_leaf.append(f1_val_leaf_score)\n",
    "\n",
    "            # for the F1 score for the internal nodes, we need to first turn the probabilities\n",
    "            # into predictions using our threshold value\n",
    "            val_parent_pred_total_thresholded = torch.where(val_parent_pred_total > threshold,1.0,0.0)\n",
    "\n",
    "#             f1_val_parent_score = f1_score( val_parent_true_total.cpu(),val_parent_pred_total_thresholded.cpu(),\n",
    "#                                         average='weighted',zero_division=np.nan)\n",
    "            f1_val_parent_score = f1_score( val_parent_true_total.cpu(),val_parent_pred_total_thresholded.cpu(),\n",
    "                                        average='weighted',zero_division=\"warn\")\n",
    "            f1_score_val_parent.append(f1_val_parent_score)\n",
    "            \n",
    "            # check if best model\n",
    "            if accuracy_val_leaf_hist[-1] > best_accuracy:\n",
    "                best_acc = accuracy_val_leaf_hist[-1]\n",
    "                best_state_dict = copy.deepcopy(clf.state_dict())\n",
    "                best_output = copy.deepcopy(outputs_val)\n",
    "            \n",
    "        if (epoch + 1) % 1 == 0 or epoch == 0:\n",
    "            print(f'[{epoch + 1}] Training Accuracy: {accuracy_train_leaf_hist[-1]:.3f} Validation Accuracy: {accuracy_val_leaf_hist[-1]:.3f}')\n",
    "            print(f'[{epoch + 1}] Training F1 Parent: {f1_score_train_parent[-1]:.3f} Validation F1 Parent: {f1_score_val_parent[-1]:.3f}')\n",
    "            print(f'[{epoch + 1}] Training F1 Leaf: {f1_score_train_leaf[-1]:.3f} Validation F1 Leaf: {f1_score_val_leaf[-1]:.3f}')\n",
    "            print(f'Train Loss: {loss_train.item():.4f} Validation Loss: {loss_val.item():.4f}')\n",
    "            #print(f'Internal Loss: {loss_val_internal.item():.4f}')\n",
    "            #print('learning rate:', optimizer.param_groups[0][\"lr\"])\n",
    "        #end_epoch = time.time()\n",
    "        #print('epoch timer', end_epoch-start_epoch)\n",
    "        #break\n",
    "\n",
    "    print(f'Best Validation Accuracy: {best_acc:.3f}')\n",
    "    \n",
    "    # build dictionary to return values\n",
    "    marginalization_dict = {}\n",
    "    marginalization_dict['accuracy_train_leaf_hist'] = accuracy_train_leaf_hist\n",
    "    marginalization_dict['accuracy_train_internal_hist'] = accuracy_train_internal_hist\n",
    "    \n",
    "    marginalization_dict['loss_train_hist'] = loss_train_hist\n",
    "    \n",
    "    marginalization_dict['loss_train_leaf_hist'] = loss_train_leaf_hist\n",
    "    marginalization_dict['loss_train_internal_hist'] = loss_train_parents_hist\n",
    "\n",
    "    marginalization_dict['accuracy_val_leaf_hist'] = accuracy_val_leaf_hist\n",
    "    marginalization_dict['accuracy_val_internal_hist'] = accuracy_val_internal_hist\n",
    "\n",
    "    marginalization_dict['loss_val_hist'] = loss_val_hist\n",
    "\n",
    "    marginalization_dict['loss_val_leaf_hist'] = loss_val_leaf_hist\n",
    "    marginalization_dict['loss_val_internal_hist'] = loss_val_parents_hist\n",
    "    \n",
    "    marginalization_dict['f1_score_train_leaf'] = f1_score_train_leaf\n",
    "    marginalization_dict['f1_score_val_leaf'] = f1_score_val_leaf\n",
    "    \n",
    "    marginalization_dict['f1_score_train_internal'] = f1_score_train_parent\n",
    "    marginalization_dict['f1_score_val_internal'] = f1_score_val_parent\n",
    "\n",
    "    marginalization_dict['best_output'] = best_output\n",
    "    marginalization_dict['best_state_dict'] = best_state_dict\n",
    "\n",
    "    return marginalization_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0768f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "\n",
    "number_of_leafs = len(leaf_values)\n",
    "number_of_parents = len(internal_values)\n",
    "\n",
    "print('There are', number_of_leafs, 'leafs and', number_of_parents, 'parents.')\n",
    "\n",
    "print('Number of training batches is {:.2f}'.format((experiment_datapipe.shape[0]*train_percent*sub_percent)/batch_size))\n",
    "print('Number of validation batches is {:.2f}'.format((experiment_datapipe.shape[0]*val_percent*sub_percent)/batch_size))\n",
    "\n",
    "# create dataframe that only includes leaf nodes\n",
    "ontology_leaf_df = ontology_df[leaf_values]\n",
    "\n",
    "# Create dictionary to map between two different types of encoded values\n",
    "# get the encoder from the datapipe\n",
    "cell_type_encoder = experiment_datapipe.obs_encoders[\"cell_type_ontology_term_id\"]\n",
    "\n",
    "# build the dictionary of encoded values from the datapipe\n",
    "# NOTE: for cellxgene==1.15.0, I had to replace cell_type_encoder.transform(cell_type_encoder.classes_)\n",
    "# with the simpler np.arange(len(cell_type_encoder.classes_))\n",
    "# encoder_mapping_dict = dict(zip(cell_type_encoder.classes_,cell_type_encoder.transform(cell_type_encoder.classes_)))\n",
    "encoder_mapping_dict = dict(zip(cell_type_encoder.classes_,np.arange(len(cell_type_encoder.classes_))))\n",
    "\n",
    "# build the dictionary mapping from encoder_mapping_dict (keys) to mapping_dict (values)\n",
    "encoding_mapper = {}\n",
    "for cell_term in encoder_mapping_dict.keys():\n",
    "    encoding_mapper[encoder_mapping_dict[cell_term]] = mapping_dict[cell_term]\n",
    "\n",
    "# set the prediction threshold for internal nodes\n",
    "threshold = 0.5 #0.8\n",
    "print('Prediction threshold for internal nodes is', threshold)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "marginalization_dict = marginalization_classification_manual_batch(train_dataloader,val_dataloader,\n",
    "                                                                   num_epochs, ontology_leaf_df, batch_size,\n",
    "                                                                  internal_values,mapping_dict,\n",
    "                                                                  ontology_df,threshold, cell_parent_mask,encoding_mapper)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "time_minutes = (end-start)/60.\n",
    "time_hours = (end-start)/3600.\n",
    "\n",
    "print(f'Run time for {num_epochs} epochs was {time_minutes:.2f} minutes ({time_hours:.2f} hours)')\n",
    "\n",
    "print(\"batches:\", int((experiment_datapipe.shape[0]*train_percent*sub_percent)/batch_size))\n",
    "\n",
    "print(\"soma chunk size:\", soma_chunk_size)\n",
    "print(\"batch size:\", batch_size)\n",
    "print(\"sub percent:\", sub_percent)\n",
    "print(\"leaf weighting:\", leaf_weight)\n",
    "print(\"neurons:\")\n",
    "print(hidden_layer_1)\n",
    "print(hidden_layer_2)\n",
    "print(hidden_layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73a007a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# graph the losses we stored during training\n",
    "plt.scatter(x_vals, batch_losses, s=2)\n",
    "\n",
    "# graph vertical lines where each epoch starts\n",
    "for i in range(num_epochs):\n",
    "    plt.axvline(x = i * int((experiment_datapipe.shape[0]*train_percent*sub_percent)/batch_size), color = 'b')\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31be627",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_datapipe.obs_encoders['cell_type_ontology_term_id'].classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb3866",
   "metadata": {},
   "source": [
    "#### Testing running times per batch\n",
    "Note that I'm only running a handful of batches, so this probbably does not account for any extra time needed to load the next batch of data from disk. That would require a longer run, so soma_chunck_size might not really matter here. And there is some variability in how long each batch takes to run. I'm not sure if this is related to disk streaming or not. This is for the full set of 2.7 million cells\n",
    "\n",
    "|batch size | soma chunk size| time per batch (s) | # of batches | time per 1 training epoch (hr) |\n",
    "| -------- | ------- |------ | ----- | ----- |\n",
    "|8192 | 10,000 | 190 | 266 | 14\n",
    "| 32768 | 10,000 | 600 | 66 | 11\n",
    "| 8192 | 20,000 | 180 | 266 | 14\n",
    "| 4096 | 20,000 | 45 | 532 | 6.7\n",
    "| 1024 | 20,000 | 15 | 2129 | 8.8\n",
    "| 512 | 20,000 |  4.5 | 4259  | 5.3\n",
    "| 256 | 20,000 | 1.4  | 8518  | 3.3\n",
    "| 256 | 30,000 |  1.8 | 8518   | 4.4\n",
    "| 128 | 30,000 | 1.06  | 17037  | 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c9a904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36cf22a5",
   "metadata": {},
   "source": [
    "## Save portions of the modeling and visualize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4945eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get today's date for saving information about this model\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1b92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model parameters to a file\n",
    "\n",
    "#marginalization_dict['best_state_dict']\n",
    "\n",
    "model_title = today + '_best_model'\n",
    "\n",
    "torch.save(marginalization_dict['best_state_dict'],model_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a62a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the loss/accuracy/f1 scores to a file\n",
    "\n",
    "cols_to_save = ['accuracy_train_leaf_hist','loss_train_hist', \n",
    "                'loss_train_leaf_hist', 'loss_train_internal_hist', \n",
    "                'accuracy_val_leaf_hist', \n",
    "                'loss_val_hist', 'loss_val_leaf_hist', 'loss_val_internal_hist', \n",
    "                'f1_score_train_leaf', 'f1_score_val_leaf',\n",
    "               'f1_score_train_internal','f1_score_val_internal']\n",
    "\n",
    "results_dict = {key: marginalization_dict[key] for key in cols_to_save}\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict, orient='columns')\n",
    "\n",
    "results_df.insert(0,'epoch',range(1,num_epochs+1))\n",
    "\n",
    "results_title = today + '_results.csv'\n",
    "\n",
    "\n",
    "results_df.to_csv(results_title,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginalization_dict['loss_val_hist']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c32fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_title = today + '_results.png'\n",
    "\n",
    "# if you want to save the plot\n",
    "plot_results(marginalization_dict,num_epochs,save_title=plot_title)\n",
    "\n",
    "# just display the plot\n",
    "#plot_results(marginalization_dict,num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcb849b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d4dcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c82cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1592e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21039bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a4ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98802bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be27cda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bd2142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_encoder.classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mccell_v4",
   "language": "python",
   "name": "mccell_v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
