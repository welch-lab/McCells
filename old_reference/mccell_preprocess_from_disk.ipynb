{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb5ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install jmespath\n",
    "# !pip3 install anndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7fdaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install jmespath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bce49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e960408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "import cellxgene_census\n",
    "\n",
    "import pronto\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pronto.warnings.ProntoWarning)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import copy\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from torcheval.metrics.functional import multilabel_accuracy\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b883e3",
   "metadata": {},
   "source": [
    "## Load Cell MetatData from the Census. \n",
    "\n",
    "Great-lakes does not have internet access, so we pull in the data outside first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76fc4e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene and cell type info stored on Turbo\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a235181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cell type list\n",
    "# and build the cell metadata value filter\n",
    "cell_type_list_name = 'cell_type_list.txt' # hematopoietic cells\n",
    "# cell_type_list_name = 'cell_type_list_full.txt' # full cell census\n",
    "with open(cell_type_list_name,'rb') as fp:\n",
    "    cell_type_list = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a2a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "census = cellxgene_census.open_soma(uri = \"/scratch/sigbio_project_root/sigbio_project25/jrepucci/mccell_data/soma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f11f76b",
   "metadata": {},
   "source": [
    "Filter out rarer cell types using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c177411",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = census[\"census_data\"][\"homo_sapiens\"]\n",
    "exp_pd = experiment.obs.read().concat().to_pandas()\n",
    "cell_type_counts = exp_pd[\"cell_type_ontology_term_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f816152",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_thresh = 5000\n",
    "good_mask = cell_type_counts > min_thresh\n",
    "\n",
    "valid_cell_types = cell_type_counts[good_mask].keys().to_list()\n",
    "\n",
    "# intersection_cell_types will be the new cell type list that we add to the obs_val_filter.\n",
    "# It is the intersection of all cell types with more than 5000 cells in the dataset and\n",
    "# the cell_type_list we get from structure_cellcensus_query.ipynb.\n",
    "# We will pickle this list to a .txt file and then read it for use in model_for_disk.ipynb\n",
    "intersection_cell_types = list(set(valid_cell_types) & set(cell_type_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f3a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "obs_val_filter = '''assay == \"10x 3\\' v3\" and is_primary_data == True and cell_type_ontology_term_id in {}'''.format(intersection_cell_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c9d9e",
   "metadata": {},
   "source": [
    "Now, get the metadata for our search. We can then use this to preprocess everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8d948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = ['cell_type_ontology_term_id']\n",
    "\n",
    "\n",
    "cell_obs_metadata = (\n",
    "    census[\"census_data\"][\"homo_sapiens\"].obs.read(value_filter = obs_val_filter,\n",
    "                                                   column_names=target_column).concat().to_pandas()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7fbaecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_type_ontology_term_id</th>\n",
       "      <th>assay</th>\n",
       "      <th>is_primary_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CL:0000763</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CL:0000763</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CL:0000763</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CL:0000763</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CL:0000542</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705363</th>\n",
       "      <td>CL:0000860</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705364</th>\n",
       "      <td>CL:0000860</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705365</th>\n",
       "      <td>CL:0000860</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705366</th>\n",
       "      <td>CL:0000940</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705367</th>\n",
       "      <td>CL:0000623</td>\n",
       "      <td>10x 3' v3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2705368 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        cell_type_ontology_term_id      assay  is_primary_data\n",
       "0                       CL:0000763  10x 3' v3             True\n",
       "1                       CL:0000763  10x 3' v3             True\n",
       "2                       CL:0000763  10x 3' v3             True\n",
       "3                       CL:0000763  10x 3' v3             True\n",
       "4                       CL:0000542  10x 3' v3             True\n",
       "...                            ...        ...              ...\n",
       "2705363                 CL:0000860  10x 3' v3             True\n",
       "2705364                 CL:0000860  10x 3' v3             True\n",
       "2705365                 CL:0000860  10x 3' v3             True\n",
       "2705366                 CL:0000940  10x 3' v3             True\n",
       "2705367                 CL:0000623  10x 3' v3             True\n",
       "\n",
       "[2705368 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cell_obs_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e81be",
   "metadata": {},
   "source": [
    "## Load the Cell Ontology\n",
    "\n",
    "The Cell Ontology also needs to be loaded for access. \n",
    "\n",
    "You can visualize the ontology using https://www.ebi.ac.uk/ols4/ontologies/cl\n",
    "\n",
    "And you can download the ontology file here: https://obofoundry.org/ontology/cl.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6015ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = pronto.Ontology.from_obo_library('cl.owl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a5a4e",
   "metadata": {},
   "source": [
    "## Data and Ontology Preprocessing\n",
    "\n",
    "To prepare the data for modeling, we need to perform some preprocessing on the data and the ontology. We'll use three functions to make this happen. Full descriptions of these functions can be found in the functions. \n",
    "\n",
    "\n",
    "- set_internal_node_values: build a dictionary to set which internal nodes are to be used in the loss calculation for internal nodes in the data\n",
    "\n",
    "- build_parent_mask: builds a masking matrix to use for masking internal node loss values\n",
    "\n",
    "- preprocess_data_ontology: this function encodes the AnnData object, splits apart the target values and primary data, and calculates some important variables from the Cell Ontology for later use \n",
    "\n",
    "- transform_data: transforms the data with log(1+x)\n",
    "\n",
    "- split_format_data: splits the data into train and validation sets, and moves the variables to PyTorch tensors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4ed19f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_internal_node_values(internal_values,all_parent_nodes):\n",
    "    '''\n",
    "    Creates a dictionary where each key is an internal cell type and the values are the cell types\n",
    "    we want to include when calculating the loss. We do not want to consider direct descendents of the\n",
    "    internal cell type, so those are removed. \n",
    "    \n",
    "    In other words, when calculating the loss for an internal node, we want to include all internal \n",
    "    nodes in the ontology EXCEPT those that are direct descendants of the target internal node. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    internal_values : list\n",
    "        list of internal values that are included in the dataset\n",
    "            \n",
    "    all_parent_nodes : list\n",
    "        from the dataset, a list of parent nodes in the ontology. Used to remove portions of\n",
    "        the Ontology where we do not have child data\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    parent_dict : dictionary\n",
    "        keys are internal_values and values are all internal cell ontology terms EXCEPT descendents \n",
    "        of the internal value. The internal value is always included\n",
    "    '''\n",
    "    \n",
    "    parent_dict = {}\n",
    "\n",
    "    # loop through each value to calculate the values to include in parent_dict for that\n",
    "    # internal value\n",
    "    for internal_node in internal_values:\n",
    "        # 1) get the children of this internal_node\n",
    "        child_nodes = []\n",
    "        for term in cl[internal_node].subclasses(distance=None,with_self=False).to_set():\n",
    "            child_nodes.append(term.id)\n",
    "        \n",
    "        # 2) remove those values from all_parent_nodes\n",
    "        cell_types_to_include = [x for x in all_parent_nodes if x not in child_nodes]\n",
    "        \n",
    "        # 3) create dictionary\n",
    "        parent_dict[internal_node] = cell_types_to_include\n",
    "    \n",
    "    return(parent_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22715b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parent_mask(leaf_values,internal_values,ontology_df,parent_dict):\n",
    "    '''\n",
    "    Function to build a masking matrix for use when calculating the internal loss\n",
    "    \n",
    "    Uses parent_dict to denote, for internal cell types, which parents to include in the loss\n",
    "    calculation. \n",
    "    \n",
    "    Parameters\n",
    "    -------\n",
    "    leaf_values : list\n",
    "        list composed of all leaf values included in the dataset\n",
    "        includes internal nodes that do not have sub-values in the dataset, and thus are\n",
    "        treated an leaf nodes\n",
    "\n",
    "    internal_values : list\n",
    "        list composed of interanal nodes in the dataset\n",
    "\n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are all leafs in portion of ontology being queried. \n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "        \n",
    "    parent_dict : dictionary\n",
    "        keys are internal_values and values are all internal cell ontology terms EXCEPT descendents \n",
    "        of the internal value. The internal value is always included\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cell_parents_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "    \n",
    "    '''\n",
    "    num_leafs = len(leaf_values)\n",
    "    num_parents = ontology_df.shape[0]\n",
    "\n",
    "    # internal_values are included as column values AND rows\n",
    "\n",
    "\n",
    "    # for cell_parents_to_include, each column is a cell type included in the\n",
    "    # dataset, so it is length = len(leaf_values) + len(internal_values)\n",
    "    # the row values are the total number of parents included for the dataset \n",
    "    # for each internal value, we need to pick (1/0) if we include that parent\n",
    "    # for the loss. For this, we reference parents_dict\n",
    "    # WHAT is the order of the cell IDs for the rows???? This is important\n",
    "    # This needs to match what we are already doing later, so let's go figure that out FIRST. \n",
    "\n",
    "    # for the leaf values, we want to include ALL parents in the \n",
    "    # loss calculation. So, we initialize the tensor as a ones tensor\n",
    "    # based on the number of leaf values and the number of parents\n",
    "    cell_parents_mask = torch.ones(num_parents,num_leafs)\n",
    "\n",
    "    # now we can deal with the internal values. For these, we will not\n",
    "    # include all parents. We will use parent_dict to select which to include\n",
    "\n",
    "\n",
    "    # first, get a list of all the parents. The ordering of this list\n",
    "    # is used later to propogate probabilities up the ontology.\n",
    "    list_of_parents = ontology_df.index.tolist()\n",
    "\n",
    "    # now, we need to loop through each internal value\n",
    "    # internal_values is ordered as -9999 + n\n",
    "    # this will be helpful later when we need to pull these values out. \n",
    "    # so the columns here are ordered at 0 to (number of leaf values), then -9999\n",
    "    # to (number of internal values)\n",
    "\n",
    "    for cell_id in internal_values:\n",
    "        # get the list of parent cell IDs we want to include for this\n",
    "        # particular internal_values\n",
    "        parent_list_for_cell = parent_dict[cell_id]\n",
    "\n",
    "        # loop through the parent_list_for cell, create a new binary list where\n",
    "        # list is 1 if the parent is in the list_of_parents, otherwise 0\n",
    "        parent_binary_list = [1 if parent in parent_list_for_cell else 0 for parent in list_of_parents]\n",
    "\n",
    "        # convert the list to a tensor and reshape for concatenation\n",
    "        parent_binary_tensor = torch.tensor(parent_binary_list).reshape(-1,1)\n",
    "\n",
    "        # append to cell_parents_to_include. \n",
    "        # we append along columns\n",
    "        cell_parents_mask = torch.cat((cell_parents_mask,parent_binary_tensor),1)\n",
    "\n",
    "    return(cell_parents_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a45b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_ontology(labels, target_column,upper_limit = None, cl_only = False, include_leafs = False):\n",
    "    '''\n",
    "    This function perfroms preprocessing on ann AnnData object to prepare it for modelling. It will encode the \n",
    "    target column and returns x_data and y_data for modelling\n",
    "    \n",
    "    This function also preprocesses the ontology to build a pandas dataframe that can be used to \n",
    "    calculate predicted probabilities. This will enable simple matrix multiplication to calculate\n",
    "    probabilities and loss.\n",
    "    \n",
    "    Can have an upper limit to the ontology if upper_limit is set\n",
    "    \n",
    "    \n",
    "    Assumes there is an active census object already open as cl. \n",
    "\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    labels : Pandas DataFrame\n",
    "        DataFrame from census.obs.read() \n",
    "        \n",
    "    target_column : string\n",
    "        string of target column (from cell metadata) to encode\n",
    "     \n",
    "    upper_limit : string\n",
    "        if you want to specify an upper limit in the ontology, set this to \n",
    "        the upper limit (inclusive)\n",
    "        Default: None (no limit to ontology)\n",
    "        \n",
    "    cl_only : boolean\n",
    "        option to only include the Cell Ontology (CL) in the dataframe\n",
    "        True means only those cell IDs that start with CL are included\n",
    "        Default: False\n",
    "        \n",
    "    include_leafs : boolean\n",
    "        option to include leafs in the list of parent cell IDs\n",
    "        Default is False because we are calculating the leaf loss differently\n",
    "        Default: False\n",
    "        \n",
    "    Returns\n",
    "    -------        \n",
    "    mapping_dict : Dictionary\n",
    "        dictionary mapping the Cell Ontology IDs (keys) to the encoded values (values)\n",
    "        Values >= 0 are leaf nodes\n",
    "        Values < 0 are internal nodes\n",
    "\n",
    "    leaf_values : list\n",
    "        list composed of all leaf values included in the dataset\n",
    "        includes internal nodes that do not have sub-values in the dataset, and thus are\n",
    "        treated an leaf nodes\n",
    "\n",
    "    internal_values : list\n",
    "        list composed of interanal nodes in the dataset\n",
    "\n",
    "    ontology_df : pandas dataframe\n",
    "        pandas dataframe where indices (rows) are all leaf and parent cell IDs from the portion of \n",
    "        the ontology being queried, and columns are all leafs in portion of ontology being queried. \n",
    "        \n",
    "        Dataframe is binary. For each parent node, element = 1 if parent node is an ancestor\n",
    "        of corresponding leaf node.\n",
    "        \n",
    "    parent_dict : dictionary\n",
    "        keys are internal_values and values are all cell ontology terms within the same distance\n",
    "        from the top node. \n",
    "        \n",
    "    cell_parent_mask : tensor\n",
    "        tensor of shape ik, where i = parent IDs and k = each cell type in the dataset\n",
    "        binary tensor where 1 means for that cell type, that parent ID will be included\n",
    "        in the internal loss calculation\n",
    "        and 0 means for that cell type, that parent ID is excluded in the internal loss\n",
    "        calculation\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # select the labels. \n",
    "    #labels = obs_metadata[target_column]\n",
    "    \n",
    "    # encode the target column\n",
    "    #lb = LabelEncoder()\n",
    "    #labels['encoded_labels'] = lb.fit_transform(labels[target_column])\n",
    "    \n",
    "    # we want to only encode the targets that are leafs. We will leave \n",
    "    # internal nodes as the CL number in order to assist with masking \n",
    "    # the appropriate parent nodes \n",
    "    # first, get list of all cell values\n",
    "    all_cell_values = labels[target_column].astype('category').unique().to_list()\n",
    "    \n",
    "    # identify which values are leafs\n",
    "    # we use positive number for leaf values\n",
    "    # and negative number for internal nodes\n",
    "    mapping_dict = {}\n",
    "    leaf_values = []\n",
    "    internal_values = []\n",
    "    encoded_leaf_val = 0\n",
    "    encoded_internal_val = -9999\n",
    "    for term in all_cell_values:\n",
    "        if cl[term].is_leaf():\n",
    "            mapping_dict[term] = encoded_leaf_val\n",
    "            leaf_values.append(term)\n",
    "            encoded_leaf_val += 1\n",
    "        else:\n",
    "            # check if internal values have associated sub-values in the dataset\n",
    "            #    sub-values do not have to be leafs\n",
    "            # if so, add value as internal values\n",
    "            # if not, prune ontology so consider \n",
    "            term_subvalues = []\n",
    "            # get leaf values of this term\n",
    "            for sub_term in cl[term].subclasses(distance=None,with_self=False).to_set():\n",
    "                    term_subvalues.append(sub_term.id)\n",
    "            \n",
    "            # get values in all_call_values in term_leafs\n",
    "            intersection_list = list(set(all_cell_values).intersection(term_subvalues))\n",
    "            if len(intersection_list) == 0:\n",
    "                mapping_dict[term] = encoded_leaf_val\n",
    "                leaf_values.append(term)\n",
    "                encoded_leaf_val += 1\n",
    "            else:\n",
    "                mapping_dict[term] = encoded_internal_val\n",
    "                internal_values.append(term)\n",
    "                encoded_internal_val += 1            \n",
    "            \n",
    "            \n",
    "    # use the leaf_mapping_dict to \n",
    "    labels['encoded_labels'] = labels[target_column].map(mapping_dict)\n",
    "    \n",
    "    #x_data = adata.X.copy()\n",
    "    #y_data = labels['encoded_labels']\n",
    "    \n",
    "    #########\n",
    "    # now get a list of all parent nodes for each value in the dataset\n",
    "    # if we want to include leafs, set with_self= True\n",
    "    # else, set with_self = False\n",
    "    \n",
    "    all_parent_nodes = []\n",
    "    for target in all_cell_values:\n",
    "        for term in cl[target].superclasses(distance=None,with_self=include_leafs).to_set():\n",
    "            all_parent_nodes.append(term.id)\n",
    "            #if target == 'CL:0000904':\n",
    "            #    print(term)\n",
    "            \n",
    "    # ensure that we do not have duplicate values\n",
    "    all_parent_nodes = list(set(all_parent_nodes))\n",
    "\n",
    "    # select only the Cell Ontology IDs if cl_only = True\n",
    "    if cl_only:\n",
    "        all_parent_nodes = [x for x in all_parent_nodes if x.startswith('CL')]\n",
    "    \n",
    "    # if there is an upper limit, \n",
    "    if upper_limit is not None:\n",
    "        # get upper limit nodes\n",
    "        upper_limit_nodes = []\n",
    "        for term in cl[upper_limit].superclasses(distance=None,with_self=False).to_set():\n",
    "            upper_limit_nodes.append(term.id)\n",
    "\n",
    "        # remove these nodes from the parent_nodes list\n",
    "        all_parent_nodes = [x for x in all_parent_nodes if x not in upper_limit_nodes]\n",
    "        \n",
    "    # create a dictionary that maps parents to reduce the ontology_df when\n",
    "    # dealing with internal nodes\n",
    "    #parent_dict = set_internal_node_relationships_by_depth(internal_values,upper_limit,all_parent_nodes)\n",
    "    parent_dict = set_internal_node_values(internal_values,all_parent_nodes)\n",
    "    \n",
    "    # create the dataframe\n",
    "    # use all_cell_values for the columns, because we need both leafs and\n",
    "    # internals nodes for mapping\n",
    "    ontology_df = pd.DataFrame(data=0, index = all_parent_nodes,\n",
    "                                              columns = all_cell_values)\n",
    "    \n",
    "    # populate the dataframe with 1 if column is a sub-node \n",
    "    # for that particular cell ID\n",
    "    # with_self = True because we need to include the leafs here\n",
    "    for cell_id in ontology_df.index:\n",
    "        for term in cl[cell_id].subclasses(distance=None,with_self=True).to_set():\n",
    "            if term.id in ontology_df.columns:\n",
    "                ontology_df.loc[cell_id,[term.id]] = [1]\n",
    "\n",
    "    # create a dictionary that maps parents to reduce the ontology_df when\n",
    "    # dealing with internal nodes\n",
    "    #parent_dict = {}\n",
    "    #for parent in internal_values:\n",
    "    #    super_parent_list = []\n",
    "    #    for term in cl[parent].superclasses(distance=None,with_self=True).to_set():\n",
    "    #         if term.id in all_parent_nodes:\n",
    "    #            super_parent_list.append(term.id)\n",
    "    #    parent_dict[parent] = super_parent_list\n",
    "\n",
    "    # build a matrix used to mask parent values\n",
    "    cell_parent_mask = build_parent_mask(leaf_values,internal_values,ontology_df,parent_dict)\n",
    "    \n",
    "    return(mapping_dict, leaf_values, internal_values, ontology_df, parent_dict, cell_parent_mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb613d",
   "metadata": {},
   "source": [
    "## Main Loop For Preprocessing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcd264a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocess data and ontology\n",
      "Preprocessing complete. There are 70 leaf values and 46 internal values.\n"
     ]
    }
   ],
   "source": [
    "target_column = 'cell_type_ontology_term_id'\n",
    "\n",
    "upper_limit = 'CL:0000988' # leukocyte = 738, hematopoietic = 988\n",
    "# upper_limit = 'CL:0000000'\n",
    "\n",
    "print('start preprocess data and ontology')\n",
    "mapping_dict, leaf_values,internal_values, \\\n",
    "    ontology_df, parent_dict, cell_parent_mask =  preprocess_data_ontology(cell_obs_metadata, target_column,\n",
    "                                                                           upper_limit = upper_limit, \n",
    "                                                                 cl_only = True, include_leafs = False)\n",
    "\n",
    "###del adata\n",
    "\n",
    "# create dataframe that only includes leaf nodes\n",
    "ontology_leaf_df = ontology_df[leaf_values]\n",
    "\n",
    "\n",
    "print('Preprocessing complete. There are {0} leaf values and {1} internal values.'.format(len(leaf_values),len(internal_values)\n",
    "                                                                                         ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da32907",
   "metadata": {},
   "source": [
    "## Save Results of Preprocessing to Disk for Use in Modeling\n",
    "\n",
    "- cell_parent_mask\n",
    "- Mapping_dict\n",
    "- Ontology_df\n",
    "- Internal_values\n",
    "- leaf_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfdedafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to directory you want to save the results in\n",
    "os.chdir('/nfs/turbo/umms-welchjd/mccell/preprocessing_outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4813a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get today's date for saving information about this model\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88f190e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save information needed for testing external models \n",
    "\n",
    "ontology_df_name = today + '_ontology_df.csv'\n",
    "ontology_df.to_csv(ontology_df_name)\n",
    "\n",
    "mapping_dict_name = today + '_mapping_dict_df.csv'\n",
    "mapping_dict_df = pd.DataFrame.from_dict(mapping_dict,orient='index')\n",
    "mapping_dict_df.to_csv(mapping_dict_name)\n",
    "\n",
    "celltypes_name = today + \"_valid_cell_types.txt\"\n",
    "\n",
    "leaf_values_name = today + '_leaf_values'\n",
    "internal_values_name = today + '_internal_values'\n",
    "\n",
    "with open(celltypes_name, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(intersection_cell_types, fp)\n",
    "\n",
    "with open(leaf_values_name, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(leaf_values, fp)\n",
    "    \n",
    "with open(internal_values_name, \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(internal_values, fp)\n",
    "\n",
    "    \n",
    "cell_parent_mask_name = today + '_cell_parent_mask.pt'\n",
    "torch.save(cell_parent_mask,cell_parent_mask_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66332828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mccell_v4",
   "language": "python",
   "name": "mccell_v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
