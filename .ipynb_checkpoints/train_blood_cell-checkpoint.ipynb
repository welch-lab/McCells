{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /home/jingqiao/real_McCell/data/processed for date 2025-10-17\n",
      "Loading cached ontology from /home/jingqiao/real_McCell/data/processed/ontology.pkl...\n",
      "Ontology loaded successfully.\n",
      "\n",
      "All data artifacts loaded successfully.\n",
      "Loaded 141 cell types.\n",
      "  - 41 leaf nodes\n",
      "  - 100 internal nodes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Imports from our project\n",
    "from src.utils.paths import PROJECT_ROOT\n",
    "from src.utils.ontology_utils import load_ontology  # Still need this to access term names\n",
    "\n",
    "# --- 1. Load Preprocessed Data Artifacts ---\n",
    "# Instead of running preprocessing, we now load the files created by `run_preprocessing.py`.\n",
    "\n",
    "# Hardcoded date for loading the preprocessed files\n",
    "DATE = '2025-10-17'\n",
    "PROCESSED_DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "print(f\"Loading data from: {PROCESSED_DATA_DIR} for date {DATE}\")\n",
    "\n",
    "# Load the ontology object to get term names for printing\n",
    "cl = load_ontology()\n",
    "\n",
    "# Load DataFrames\n",
    "marginalization_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_marginalization_df.csv\", index_col=0)\n",
    "parent_child_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_parent_child_df.csv\", index_col=0)\n",
    "exclusion_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_exclusion_df.csv\", index_col=0)\n",
    "\n",
    "# Load mapping_dict\n",
    "mapping_dict_df = pd.read_csv(PROCESSED_DATA_DIR / f\"{DATE}_mapping_dict_df.csv\", index_col=0)\n",
    "# The DataFrame was saved with CL numbers as the index and integer mappings in the first column\n",
    "mapping_dict = pd.Series(mapping_dict_df.iloc[:, 0].values, index=mapping_dict_df.index).to_dict()\n",
    "\n",
    "# Load leaf and internal values\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_leaf_values.pkl\", \"rb\") as fp:\n",
    "    leaf_values = pickle.load(fp)\n",
    "with open(PROCESSED_DATA_DIR / f\"{DATE}_internal_values.pkl\", \"rb\") as fp:\n",
    "    internal_values = pickle.load(fp)\n",
    "\n",
    "print(\"\\nAll data artifacts loaded successfully.\")\n",
    "print(f\"Loaded {len(mapping_dict)} cell types.\")\n",
    "print(f\"  - {len(leaf_values)} leaf nodes\")\n",
    "print(f\"  - {len(internal_values)} internal nodes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and filtering gene list...\n",
      "Querying 141 cell types and 500 protein-coding genes.\n"
     ]
    }
   ],
   "source": [
    "import cellxgene_census\n",
    "import tiledbsoma as soma\n",
    "from tiledbsoma_ml import ExperimentDataset, experiment_dataloader\n",
    "\n",
    "# Get all cell types from our mapping dict to build the query\n",
    "all_cell_values = list(mapping_dict.keys())\n",
    "\n",
    "# --- Create Filters for SOMA Query ---\n",
    "\n",
    "# Use a subset of genes for speed\n",
    "print(\"Loading and filtering gene list...\")\n",
    "# Assuming the gene list file is in a relative path\n",
    "biomart = pd.read_csv(PROJECT_ROOT / \"hpc_workaround/data/mart_export.txt\")\n",
    "coding_only = biomart[biomart['Gene type'] == 'protein_coding']\n",
    "full_gene_list = coding_only['Gene stable ID'].tolist()\n",
    "num_genes_to_use = 500\n",
    "gene_list = full_gene_list[:num_genes_to_use]\n",
    "\n",
    "# Create the 'value_filter' strings for the query\n",
    "var_value_filter = f\"feature_id in {gene_list}\"\n",
    "obs_value_filter = f\"assay == '10x 3\\\\' v3' and is_primary_data == True and cell_type_ontology_term_id in {all_cell_values}\"\n",
    "\n",
    "print(f\"Querying {len(all_cell_values)} cell types and {len(gene_list)} protein-coding genes.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening local SOMA database at: /scratch/sigbio_project_root/sigbio_project25/jingqiao/mccell-single/soma_db_homo_sapiens\n",
      "\n",
      "Total matching cells: 49872\n",
      "Training set size: 39898\n",
      "Validation set size: 9975\n",
      "\n",
      "Train dataset shape: (39898, 389)\n",
      "Validation dataset shape: (9975, 389)\n"
     ]
    }
   ],
   "source": [
    "# Point to the local SOMA database instead of the live one\n",
    "soma_uri = \"/scratch/sigbio_project_root/sigbio_project25/jingqiao/mccell-single/soma_db_homo_sapiens\"\n",
    "print(f\"Opening local SOMA database at: {soma_uri}\")\n",
    "\n",
    "with soma.open(uri=soma_uri) as experiment:\n",
    "\n",
    "    # Create the ExperimentDataset and DataLoaders using the query filters\n",
    "    with experiment.axis_query(\n",
    "        measurement_name=\"RNA\",\n",
    "        obs_query=soma.AxisQuery(value_filter=obs_value_filter),\n",
    "        var_query=soma.AxisQuery(value_filter=var_value_filter),\n",
    "    ) as query:\n",
    "        experiment_dataset = ExperimentDataset(\n",
    "            query,\n",
    "            obs_column_names=[\"cell_type_ontology_term_id\"],\n",
    "            layer_name=\"raw\",\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    "            seed=111\n",
    "        )\n",
    "\n",
    "        train_dataset, val_dataset = experiment_dataset.random_split([0.8, 0.2], seed=42)\n",
    "\n",
    "        print(f'\\nTotal matching cells: {len(experiment_dataset)}')\n",
    "        print(f'Training set size: {len(train_dataset)}')\n",
    "        print(f'Validation set size: {len(val_dataset)}')\n",
    "\n",
    "        train_dataloader = experiment_dataloader(train_dataset)\n",
    "        val_dataloader = experiment_dataloader(val_dataset)\n",
    "\n",
    "# Show a summary of the loaded train and validation datasets\n",
    "print(\"\\nTrain dataset shape:\", train_dataset.shape)\n",
    "print(\"Validation dataset shape:\", val_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# --- 1. Setup ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m\"\u001b[39m\u001b[33mcuda:0\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# The input dimension is the number of genes from our dataset object\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from src.train.model import SimpleNN\n",
    "from src.train.loss import MarginalizationLoss\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Setup ---\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# The input dimension is the number of genes from our dataset object\n",
    "input_dim = train_dataset.shape[1]\n",
    "output_dim = len(leaf_values)  # Model only predicts leaf nodes\n",
    "\n",
    "model = SimpleNN(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Instantiate the new, correct loss function with all required artifacts\n",
    "loss_fn = MarginalizationLoss(\n",
    "    marginalization_df=marginalization_df,\n",
    "    parent_child_df=parent_child_df,\n",
    "    exclusion_df=exclusion_df,\n",
    "    leaf_values=leaf_values,\n",
    "    internal_values=internal_values,\n",
    "    mapping_dict=mapping_dict,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Model, optimizer, and loss function are ready.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 2 epochs (200 batches each)...\n",
      "\n",
      "--- Epoch 1 ---\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "batches_per_epoch = 200  # Limit batches for a quick run\n",
    "batch_loss_history = []\n",
    "\n",
    "print(f\"\\nStarting training for {num_epochs} epochs ({batches_per_epoch} batches each)...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(f'\\n--- Epoch {epoch + 1} ---')\n",
    "\n",
    "    for i, (X_batch, obs_batch) in enumerate(train_dataloader):\n",
    "        if i >= batches_per_epoch:\n",
    "            break\n",
    "\n",
    "        # --- Data Preparation ---\n",
    "        X_batch = torch.from_numpy(X_batch).float().to(device)\n",
    "        label_strings = obs_batch[\"cell_type_ontology_term_id\"]\n",
    "        y_batch = torch.tensor([mapping_dict[term] for term in label_strings], device=device, dtype=torch.long)\n",
    "\n",
    "        # --- Training Step ---\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        # Our loss function returns all three components\n",
    "        total_loss, loss_leafs, loss_parents = loss_fn(outputs, y_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # --- Logging ---\n",
    "        batch_loss_history.append(total_loss.item())\n",
    "        # Print a more detailed log every 50 batches\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f'  [Batch {i + 1:3d}] Total Loss: {total_loss.item():.4f} (Leaf: {loss_leafs.item():.4f}, Parent: {loss_parents.item():.4f})')\n",
    "\n",
    "print('\\nFinished Training.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.92.136.210\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "print(socket.gethostbyname(\"cellxgene-census-public-us-west-2.s3.us-west-2.amazonaws.com\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mccell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
